{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['showbothmetrics_llava1.5_with_image_q20_i10_s0', 'llava1.5_with_image_q20_i10_s0', 'prodmetric_llava1.5_with_image_q20_i10_s0', 'vf_numeric_llava1.5_with_image_q20_i10_s0', 'contr_numeric_llava1.5_with_image_q20_i10_s0', 'avg_vf_contr_numeric_llava1.5_with_image_q20_i10_s0', 'showbothmetrics_descriptive_llava1.5_with_image_q20_i10_s0']\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import glob, os\n",
    "from itertools import combinations\n",
    "\n",
    "USER_STUDIES_DIR = \"/home/shared/vlm_rationales_eval/user_studies_data/prolific_batches/batch_interaction_data\"\n",
    "print(os.listdir(USER_STUDIES_DIR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['prolific_batches']\n",
      "Processing setting: showbothmetrics_llava1.5_with_image_q20_i10_s0\n",
      "30 participants in showbothmetrics_llava1.5_with_image_q20_i10_s0\n",
      "Processing setting: prodmetric_llava1.5_with_image_q20_i10_s0\n",
      "31 participants in prodmetric_llava1.5_with_image_q20_i10_s0\n",
      "Processing setting: vf_numeric_llava1.5_with_image_q20_i10_s0\n",
      "28 participants in vf_numeric_llava1.5_with_image_q20_i10_s0\n",
      "Processing setting: contr_numeric_llava1.5_with_image_q20_i10_s0\n",
      "30 participants in contr_numeric_llava1.5_with_image_q20_i10_s0\n",
      "Processing setting: avg_vf_contr_numeric_llava1.5_with_image_q20_i10_s0\n",
      "31 participants in avg_vf_contr_numeric_llava1.5_with_image_q20_i10_s0\n",
      "\n",
      "Overlap between settings:\n",
      "Overlap between showbothmetrics_llava1.5_with_image_q20_i10_s0 and prodmetric_llava1.5_with_image_q20_i10_s0: 0 participants\n",
      "Overlap between showbothmetrics_llava1.5_with_image_q20_i10_s0 and vf_numeric_llava1.5_with_image_q20_i10_s0: 1 participants\n",
      "Overlap between showbothmetrics_llava1.5_with_image_q20_i10_s0 and contr_numeric_llava1.5_with_image_q20_i10_s0: 0 participants\n",
      "Overlap between showbothmetrics_llava1.5_with_image_q20_i10_s0 and avg_vf_contr_numeric_llava1.5_with_image_q20_i10_s0: 0 participants\n",
      "Overlap between prodmetric_llava1.5_with_image_q20_i10_s0 and vf_numeric_llava1.5_with_image_q20_i10_s0: 0 participants\n",
      "Overlap between prodmetric_llava1.5_with_image_q20_i10_s0 and contr_numeric_llava1.5_with_image_q20_i10_s0: 0 participants\n",
      "Overlap between prodmetric_llava1.5_with_image_q20_i10_s0 and avg_vf_contr_numeric_llava1.5_with_image_q20_i10_s0: 0 participants\n",
      "Overlap between vf_numeric_llava1.5_with_image_q20_i10_s0 and contr_numeric_llava1.5_with_image_q20_i10_s0: 0 participants\n",
      "Overlap between vf_numeric_llava1.5_with_image_q20_i10_s0 and avg_vf_contr_numeric_llava1.5_with_image_q20_i10_s0: 0 participants\n",
      "Overlap between contr_numeric_llava1.5_with_image_q20_i10_s0 and avg_vf_contr_numeric_llava1.5_with_image_q20_i10_s0: 0 participants\n",
      "\n",
      "Participants present in all three settings: 0\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import glob, os\n",
    "from itertools import combinations\n",
    "\n",
    "USER_STUDIES_DIR = \"/home/shared/vlm_rationales_eval/user_studies_data/\"\n",
    "print(os.listdir(USER_STUDIES_DIR))\n",
    "settings = [\n",
    "    'showbothmetrics_llava1.5_with_image_q20_i10_s0',\n",
    "    'prodmetric_llava1.5_with_image_q20_i10_s0',\n",
    "    'vf_numeric_llava1.5_with_image_q20_i10_s0',\n",
    "    'contr_numeric_llava1.5_with_image_q20_i10_s0', \n",
    "    'avg_vf_contr_numeric_llava1.5_with_image_q20_i10_s0'\n",
    "]\n",
    "\n",
    "# Dictionary to hold the set of user_ids for each setting.\n",
    "settings_user_ids = {}\n",
    "\n",
    "for setting in settings:\n",
    "    print(f\"Processing setting: {setting}\")\n",
    "    pattern = os.path.join(USER_STUDIES_DIR, \"prolific_batches\", \"batch_interaction_data\", setting, \"*.json\")\n",
    "    files = glob.glob(pattern)\n",
    "    user_ids = set()\n",
    "    \n",
    "    for file in files:\n",
    "        with open(file) as f:\n",
    "            batch_data = json.load(f)\n",
    "            # Add all user_ids from the current JSON file.\n",
    "            user_ids.update(batch_data.keys())\n",
    "    \n",
    "    settings_user_ids[setting] = user_ids\n",
    "    print(f\"{len(user_ids)} participants in {setting}\")\n",
    "\n",
    "# Compare overlaps between each pair of settings.\n",
    "print(\"\\nOverlap between settings:\")\n",
    "for s1, s2 in combinations(settings, 2):\n",
    "    overlap = settings_user_ids[s1].intersection(settings_user_ids[s2])\n",
    "    print(f\"Overlap between {s1} and {s2}: {len(overlap)} participants\")\n",
    "    \n",
    "# Optionally, check for participants common to all three settings.\n",
    "triple_overlap = settings_user_ids[settings[0]].intersection(settings_user_ids[settings[1]], settings_user_ids[settings[2]])\n",
    "print(f\"\\nParticipants present in all three settings: {len(triple_overlap)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting: avg_vf_contr_numeric_llava1.5_with_image_q20_i10_s0\n",
      "31 total sessions loaded from 10 files\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import glob, os\n",
    "\n",
    "USER_STUDIES_DIR = \"/home/shared/vlm_rationales_eval/user_studies_data/\"\n",
    "\n",
    "# setting = 'showbothmetrics_llava1.5_with_image_q20_i10_s0'\n",
    "# setting = 'showbothmetrics_descriptive_llava1.5_with_image_q20_i10_s0'\n",
    "# setting = 'prodmetric_llava1.5_with_image_q20_i10_s0'\n",
    "# setting = 'vf_numeric_llava1.5_with_image_q20_i10_s0'\n",
    "# setting = 'contr_numeric_llava1.5_with_image_q20_i10_s0'\n",
    "setting = 'avg_vf_contr_numeric_llava1.5_with_image_q20_i10_s0'\n",
    "print(f\"Setting: {setting}\")\n",
    "files = glob.glob(f'{USER_STUDIES_DIR}/prolific_batches/batch_interaction_data/{setting}/*.json')\n",
    "data = {}\n",
    "for file in files:\n",
    "    with open(file) as f:\n",
    "        batch_data = json.load(f)\n",
    "        for user_id, session in batch_data.items():\n",
    "            if user_id in data:\n",
    "                data[user_id].append(session)  # Append to list instead of overwriting\n",
    "            else:\n",
    "                data[user_id] = [session]  # Initialize as list\n",
    "\n",
    "user_ids = list(data.keys())\n",
    "for user_id in user_ids:\n",
    "    interactions = sum([x['interactions'] for x in data[user_id]], [])\n",
    "    user_preds = np.array([interaction['user_selections']['answeronly'] for interaction in interactions])\n",
    "    unsure_rate = np.mean(user_preds == 2)\n",
    "    if unsure_rate < 0.2:\n",
    "        del data[user_id]\n",
    "\n",
    "print(f\"{sum(len(sessions) for sessions in data.values())} total sessions loaded from {len(files)} files\")\n",
    "\n",
    "all_instances = sum([session['interactions'] for sessions in data.values() for session in sessions], [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage                   \tUnsure Rate\tTotalAcc\tNotUnsureAcc\tUtility\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Answer Only              \t85.2% ± 4.0%\t8.1% ± 3.0%\t54.3% ± 14.4%\t0.013 ± 0.043\n",
      "With Explanation         \t34.2% ± 5.3%\t41.6% ± 5.5%\t63.2% ± 6.6%\t0.174 ± 0.088\n",
      "With Explanation + Quality\t6.1% ± 2.7%\t59.4% ± 5.5%\t63.2% ± 5.5%\t0.248 ± 0.104\n",
      "\n",
      "Setting: avg_vf_contr_numeric_llava1.5_with_image_q20_i10_s0\n",
      "Stage\t\tUnsure Rate\tUtility over previous stage\n",
      "Explanation\t 34.2%    \t 0.161 ± 0.088\n",
      "Quality\t\t 6.1%    \t 0.074 ± 0.070\n",
      "\n",
      "Setting: avg_vf_contr_numeric\n",
      "\t\tIncorrect\tUnsure   \tCorrect\n",
      "Incorrect \t 66\t\t40\t\t1\n",
      "Unsure    \t 6\t\t8\t\t5\n",
      "Correct   \t 3\t\t58\t\t123\n",
      "M1\tM2\tM3\tM4\n",
      "92.45%\t59.18%\t12.00%\t4.65%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the interactions\n",
    "from collections import Counter\n",
    "import math\n",
    "\n",
    "def compute_margin(p, n):\n",
    "    return 1.96*math.sqrt(p*(1-p)/n) if n > 0 else 0\n",
    "\n",
    "def compute_proportion_ci(p, n):\n",
    "    margin = compute_margin(p, n)\n",
    "    return (p - margin, p + margin)\n",
    "\n",
    "def compute_mean_ci(values):\n",
    "    n = len(values)\n",
    "    mean_val = np.mean(values)\n",
    "    std_val = np.std(values, ddof=1)\n",
    "    se = std_val/np.sqrt(n) if n > 0 else 0\n",
    "    return (mean_val - 1.96*se, mean_val + 1.96*se)\n",
    "\n",
    "def evaluate_answers(stage, instances):\n",
    "    ground_truths = np.array([1-x['question']['prediction_is_correct'] for x in instances])       # 0 means AI is correct, 1 means AI is incorrect\n",
    "    preds = np.array([x['user_selections'][stage] for x in instances])\n",
    "    true_positives = np.sum(np.logical_and(preds == 0, ground_truths == 0))\n",
    "    false_positives = np.sum(np.logical_and(preds == 0, ground_truths == 1))\n",
    "    true_negatives = np.sum(np.logical_and(preds == 1, ground_truths == 1))\n",
    "    false_negatives = np.sum(np.logical_and(preds == 1, ground_truths == 0))\n",
    "\n",
    "    unsure_rate = np.mean(preds == 2)\n",
    "    accuracy = (true_positives + true_negatives) / (true_positives + false_positives + true_negatives + false_negatives)\n",
    "    total_accuracy = (true_positives + true_negatives) / len(ground_truths)\n",
    "    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives > 0 else 0\n",
    "    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0\n",
    "    false_positive_rate = false_positives / (false_positives + true_negatives) if false_positives + true_negatives > 0 else 0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if precision + recall > 0 else 0\n",
    "\n",
    "    # Utility: 0 if user is unsure, 1 if user correctly predicts AI correctness, -1 if user incorrectly predicts AI correctness\n",
    "    individual_utilities = np.array([0 if preds[i] == 2 else 1-2*np.abs(preds[i] - ground_truths[i]) for i in range(len(preds))])\n",
    "    utility = np.mean(individual_utilities)\n",
    "    \n",
    "    # Compute 95% CIs.\n",
    "    total_accuracy_ci = compute_proportion_ci(total_accuracy, len(ground_truths))\n",
    "    unsure_rate_ci    = compute_proportion_ci(unsure_rate, len(preds))\n",
    "    \n",
    "    non_unsure = preds != 2\n",
    "    non_unsure_n = np.sum(non_unsure)\n",
    "    accuracy_ci = compute_proportion_ci(accuracy, non_unsure_n) if non_unsure_n > 0 else (0, 0)\n",
    "    \n",
    "    precision_n = true_positives + false_positives\n",
    "    precision_ci = compute_proportion_ci(precision, precision_n) if precision_n > 0 else (0, 0)\n",
    "    \n",
    "    recall_n = true_positives + false_negatives\n",
    "    recall_ci = compute_proportion_ci(recall, recall_n) if recall_n > 0 else (0, 0)\n",
    "    \n",
    "    fpr_n = false_positives + true_negatives\n",
    "    fpr_ci = compute_proportion_ci(false_positive_rate, fpr_n) if fpr_n > 0 else (0, 0)\n",
    "    \n",
    "    utility_ci = compute_mean_ci(individual_utilities)\n",
    "\n",
    "    return {\n",
    "        'total_accuracy': total_accuracy,\n",
    "        'total_accuracy_ci': total_accuracy_ci,\n",
    "        'accuracy': accuracy,\n",
    "        'accuracy_ci': accuracy_ci,\n",
    "        'precision': precision,\n",
    "        'precision_ci': precision_ci,\n",
    "        'recall': recall,\n",
    "        'recall_ci': recall_ci,\n",
    "        'false_positive_rate': false_positive_rate,\n",
    "        'fpr_ci': fpr_ci,\n",
    "        'f1': f1,\n",
    "        'unsure_rate': unsure_rate,\n",
    "        'unsure_rate_ci': unsure_rate_ci,\n",
    "        'utility': utility,\n",
    "        'utility_ci': utility_ci,\n",
    "        'preds': preds,\n",
    "        'ground_truths': ground_truths,\n",
    "        'individual_utilities': individual_utilities,\n",
    "    }\n",
    "\n",
    "answeronly_results = evaluate_answers('answeronly', all_instances)\n",
    "withexplanation_results = evaluate_answers('withexplanation', all_instances)\n",
    "withexplanationquality_results = evaluate_answers('withexplanationquality', all_instances)\n",
    "\n",
    "# print(\"Stage                   \\tUnsure Rate\\tTotalAcc\\tNotUnsureAcc\\tPrecision\\tRecall\\t\\tF1\\t\\tFPR\\t\\tUtility\")\n",
    "# print(\"-\"*140)\n",
    "# for stage, results in zip(\n",
    "#     ['Answer Only', 'With Explanation', 'With Explanation + Quality'], \n",
    "#     [answeronly_results, withexplanation_results, withexplanationquality_results]\n",
    "# ):\n",
    "#     print(f\"{stage:<25}\\t{results['unsure_rate']:.1%}\\t\\t{results['total_accuracy']:.1%}\\t\\t{results['accuracy']:.1%}\\t\\t{results['precision']:.3f}\\t\\t{results['recall']:.3f}\\t\\t{results['f1']:.3f}\\t\\t{results['false_positive_rate']:.3f}\\t\\t{results['utility']:.3f}\")\n",
    "    \n",
    "# print(\"Stage                   \\tUnsure Rate\\tTotalAcc\\tNotUnsureAcc\\tPrecision\\tRecall\\t\\tF1\\tFPR\\t\\tUtility\")\n",
    "print(\"Stage                   \\tUnsure Rate\\tTotalAcc\\tNotUnsureAcc\\tUtility\")\n",
    "# print(\"-\"*150) \n",
    "print(\"-\"*100)\n",
    "for stage_name, results in zip(\n",
    "    ['Answer Only', 'With Explanation', 'With Explanation + Quality'], \n",
    "    [answeronly_results, withexplanation_results, withexplanationquality_results]\n",
    "):\n",
    "    # Calculate margins as half the width of each CI.\n",
    "    unsure_margin    = (results['unsure_rate_ci'][1] - results['unsure_rate_ci'][0]) / 2\n",
    "    total_acc_margin = (results['total_accuracy_ci'][1] - results['total_accuracy_ci'][0]) / 2\n",
    "    acc_margin       = (results['accuracy_ci'][1] - results['accuracy_ci'][0]) / 2\n",
    "    prec_margin      = (results['precision_ci'][1] - results['precision_ci'][0]) / 2 if results['precision_ci'] != (0, 0) else 0\n",
    "    recall_margin    = (results['recall_ci'][1] - results['recall_ci'][0]) / 2 if results['recall_ci'] != (0, 0) else 0\n",
    "    fpr_margin       = (results['fpr_ci'][1] - results['fpr_ci'][0]) / 2 if results['fpr_ci'] != (0, 0) else 0\n",
    "    utility_margin   = (results['utility_ci'][1] - results['utility_ci'][0]) / 2\n",
    "\n",
    "    # For proportion metrics, we print as percentages; f1 is printed as a decimal (CI not computed); utility remains as a number.\n",
    "    print(f\"{stage_name:<25}\\t\"\n",
    "          f\"{results['unsure_rate']:.1%} ± {unsure_margin:.1%}\\t\"\n",
    "          f\"{results['total_accuracy']:.1%} ± {total_acc_margin:.1%}\\t\"\n",
    "          f\"{results['accuracy']:.1%} ± {acc_margin:.1%}\\t\"\n",
    "        #   f\"{results['precision']:.1%} ± {prec_margin:.1%}\\t\"\n",
    "        #   f\"{results['recall']:.1%} ± {recall_margin:.1%}\\t\"\n",
    "        #   f\"{results['f1']:.3f}\\t\"\n",
    "        #   f\"{results['false_positive_rate']:.1%} ± {fpr_margin:.1%}\\t\"\n",
    "          f\"{results['utility']:.3f} ± {utility_margin:.3f}\")\n",
    "    \n",
    "    # print(\"copiable results:\", end=' ')\n",
    "    # print(f\"{results['unsure_rate']:.1%} ± {unsure_margin:.1%}, \", end='')\n",
    "    # print(f\"{results['total_accuracy']:.1%} ± {total_acc_margin:.1%}, \", end='')\n",
    "    # print(f\"{results['accuracy']:.1%} ± {acc_margin:.1%}, \", end='')\n",
    "    # print(f\"{results['utility']:.3f} ± {utility_margin:.3f}\")\n",
    "\n",
    "print(\"\\nSetting:\", setting)\n",
    "print(\"Stage\\t\\tUnsure Rate\\tUtility over previous stage\")\n",
    "answeronly_individual_utilities = answeronly_results['individual_utilities']\n",
    "explanation_individual_utilities = withexplanation_results['individual_utilities']\n",
    "explanationquality_individual_utilities = withexplanationquality_results['individual_utilities']\n",
    "assert len(explanation_individual_utilities) == len(explanationquality_individual_utilities)\n",
    "assert len(answeronly_individual_utilities) == len(explanation_individual_utilities)\n",
    "answeronly_preds = answeronly_results['preds']\n",
    "explanation_preds = withexplanation_results['preds']\n",
    "explanationquality_preds = withexplanationquality_results['preds']\n",
    "assert len(explanation_preds) == len(explanationquality_preds)\n",
    "assert len(answeronly_preds) == len(explanation_preds)\n",
    "\n",
    "answeronly_unsurepreds = len([apred for apred in answeronly_preds if apred == 2])\n",
    "explanation_nonunsurepreds = len([apred for apred, epred in zip(answeronly_preds, explanation_preds) if apred == 2 & epred != 2])\n",
    "explanation_unsuredecrease = (answeronly_unsurepreds - explanation_nonunsurepreds) / answeronly_unsurepreds\n",
    "\n",
    "explanationquality_nonunsurepreds = len([apred for apred, epred in zip(answeronly_preds, explanationquality_preds) if apred == 2 & epred != 2])\n",
    "explanationquality_unsuredecrease = (answeronly_unsurepreds - explanationquality_nonunsurepreds) / answeronly_unsurepreds\n",
    "\n",
    "explanation_utility_gains = [explanation - answeronly for answeronly, explanation in zip(answeronly_individual_utilities, explanation_individual_utilities)]\n",
    "explanation_utility_gains_ci = compute_mean_ci(explanation_utility_gains)\n",
    "explanation_utility_gains_ci_margin = (explanation_utility_gains_ci[1] - explanation_utility_gains_ci[0]) / 2\n",
    "print(\"Explanation\\t\", f\"{withexplanation_results['unsure_rate']:.1%}    \\t\", f\"{np.mean(explanation_utility_gains):.3f} ± {explanation_utility_gains_ci_margin:.3f}\")\n",
    "\n",
    "\n",
    "\n",
    "quality_utility_gains = [quality - explanation for explanation, quality in zip(explanation_individual_utilities, explanationquality_individual_utilities)]\n",
    "quality_utility_gains_ci = compute_mean_ci(quality_utility_gains)\n",
    "quality_utility_gains_ci_margin = (quality_utility_gains_ci[1] - quality_utility_gains_ci[0]) / 2\n",
    "print(f\"Quality\\t\\t\", f\"{withexplanationquality_results['unsure_rate']:.1%}    \\t\", f\"{np.mean(quality_utility_gains):.3f} ± {quality_utility_gains_ci_margin:.3f}\")\n",
    "    \n",
    "def num(stage2_utility, stage3_utility):\n",
    "    return len([1 for u1, u2 in zip(explanation_individual_utilities, explanationquality_individual_utilities) if u1 == stage2_utility and u2 == stage3_utility])\n",
    "\n",
    "print(f\"\\nSetting: {setting.split('_ll')[0]}\")\n",
    "print(\"\\t\\tIncorrect\\tUnsure   \\tCorrect\")\n",
    "for i, s in zip([-1, 0, 1], ['Incorrect', 'Unsure   ', 'Correct  ']):\n",
    "    print(s, \"\\t\", '\\t\\t'.join([str(num(j, i)) for j in [-1, 0, 1]]))\n",
    "\n",
    "# M1: P(S3 != U | S2 = U)\n",
    "m1 = (num(0, 1) + num(0, -1)) / (num(0, 1) + num(0, -1) + num(0, 0))\n",
    "\n",
    "# M2: P(S3 = C | S2 = U, S3 != U)\n",
    "m2 = num(0, 1) / (num(0, 1) + num(0, -1))\n",
    "\n",
    "# M3: P(S3 != I | S2 = I)\n",
    "m3 = (num(-1, 0) + num(-1, 1)) / (num(-1, 0) + num(-1, 1) + num(-1, -1))\n",
    "\n",
    "# M4: P(S3 != C | S2 = C)\n",
    "m4 = (num(1, 0) + num(1, -1)) / (num(1, 0) + num(1, -1) + num(1, 1))\n",
    "print(\"M1\\tM2\\tM3\\tM4\")\n",
    "print(f\"{m1:.2%}\\t{m2:.2%}\\t{m3:.2%}\\t{m4:.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import glob, os\n",
    "\n",
    "USER_STUDIES_DIR = \"/home/shared/vlm_rationales_eval/user_studies_data/\"\n",
    "\n",
    "# setting = 'showbothmetrics_llava1.5_with_image_q20_i10_s0'\n",
    "# setting = 'prodmetric_llava1.5_with_image_q20_i10_s0'\n",
    "def load_data(setting):\n",
    "    #print(f\"Setting: {setting}\")\n",
    "    files = glob.glob(f'{USER_STUDIES_DIR}/prolific_batches/batch_interaction_data/{setting}/*.json')\n",
    "    data = {}\n",
    "    for file in files:\n",
    "        with open(file) as f:\n",
    "            batch_data = json.load(f)\n",
    "            for user_id, session in batch_data.items():\n",
    "                if user_id in data:\n",
    "                    print(user_id, setting, session['interactions'][0]['url_data']['uid'], data[user_id][0]['interactions'][0]['url_data']['uid'])\n",
    "                    data[user_id].append(session)  # Append to list instead of overwriting\n",
    "                else:\n",
    "                    data[user_id] = [session]  # Initialize as list\n",
    "\n",
    "    #print(f\"{sum(len(sessions) for sessions in data.values())} total sessions loaded from {len(files)} files\")\n",
    "    return data\n",
    "\n",
    "settings = [\n",
    "    'showbothmetrics', \n",
    "    'vf_numeric', \n",
    "    'contr_numeric', \n",
    "    'avg_vf_contr_numeric', \n",
    "]\n",
    "data = {setting: load_data(f\"{setting}_llava1.5_with_image_q20_i10_s0\") for setting in settings}\n",
    "\n",
    "from collections import defaultdict\n",
    "counter = defaultdict(lambda: defaultdict(int))\n",
    "for setting, user_data in data.items():\n",
    "    all_instances = sum([session['interactions'] for sessions in user_data.values() for session in sessions], [])\n",
    "    for instance in all_instances:\n",
    "        counter[instance['question']['question_id']][setting] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qids = list(counter.keys())\n",
    "qids.sort()\n",
    "print(\"QID\\t\" + '\\t'.join(settings))\n",
    "for qid in qids:\n",
    "    print(qid + \"\\t\" + '\\t\\t'.join(f'{counter[qid][setting]}' for setting in settings))\n",
    "for setting in settings:\n",
    "    counts = [counter[qid][setting] for qid in qids]\n",
    "    print(setting)\n",
    "    print(\"\\tQuestions with < 3 annotations:\", sum(1 for count in counts if count < 3))\n",
    "    print(\"\\tQuestions with > 3 annotations:\", sum(1 for count in counts if count > 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Setting             \\tQueue #\\t Study ID                 \\tProlific User ID          \\tInitial (answer-only) Unsure Rate\")\n",
    "print(\"-\"*120)\n",
    "\n",
    "for setting in settings:\n",
    "    #print(f\"Setting: {setting}\")\n",
    "    for user_id in data[setting]:\n",
    "        interactions = sum([x['interactions'] for x in data[setting][user_id]], [])\n",
    "        user_preds = np.array([interaction['user_selections']['answeronly'] for interaction in interactions])\n",
    "        unsure_rate = np.mean(user_preds == 2)\n",
    "        # print(f\"\\t{user_id}: {unsure_rate:.0%} unsure rate\")\n",
    "        if unsure_rate < 0.2:\n",
    "            # print(\"\\t\\t\", user_id, setting, data[setting][user_id][0]['interactions'][0]['url_data']['uid'])\n",
    "            #print(user_id, \"\\t\", setting, \"    \\t\", data[setting][user_id][0]['interactions'][0]['url_data']['study_id'])\n",
    "            print(setting, \"     \\t\",data[setting][user_id][0]['interactions'][0]['url_data']['uid'].split('/')[-1], \"\\t\", data[setting][user_id][0]['interactions'][0]['url_data']['study_id'], \"\\t\", user_id, \"\\t\", unsure_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9c71c9917d6ed34065e297326319142482db660d27cedb7baabed1e76e934b0b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
