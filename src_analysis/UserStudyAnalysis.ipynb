{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting: showbothmetrics_llava1.5_with_image_q20_i10_s0\n",
      "5 sessions loaded from 1 files\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import glob, os\n",
    "\n",
    "USER_STUDIES_DIR = \"/home/shared/vlm_rationales_eval/user_studies_data/\"\n",
    "\n",
    "setting = 'showbothmetrics_llava1.5_with_image_q20_i10_s0'\n",
    "print(f\"Setting: {setting}\")\n",
    "files = glob.glob(f'{USER_STUDIES_DIR}/prolific_batches/batch_interaction_data/{setting}/*.json')\n",
    "data = {}\n",
    "for file in files:\n",
    "    with open(file) as f:\n",
    "        data.update(json.load(f))\n",
    "print(f\"{len(data)} sessions loaded from {len(files)} files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_instances = sum([data[uid]['interactions'] for uid in data], [])\n",
    "len(all_instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage                   \tUnsure Rate\tAccuracy\tPrecision\tRecall\t\tF1\t\tFPR\t\tUtility\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Answer Only              \t70.0%\t\t60.0%\t\t0.600\t\t1.000\t\t0.750\t\t1.000\t\t0.060\n",
      "With Explanation         \t26.0%\t\t51.4%\t\t0.545\t\t0.857\t\t0.667\t\t0.938\t\t0.020\n",
      "With Explanation + Quality\t16.0%\t\t52.4%\t\t0.559\t\t0.792\t\t0.655\t\t0.833\t\t0.040\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the interactions\n",
    "from collections import Counter\n",
    "\n",
    "def evaluate_answers(stage, instances):\n",
    "    ground_truths = np.array([1-x['question']['prediction_is_correct'] for x in instances])       # 0 means AI is correct, 1 means AI is incorrect\n",
    "    preds = np.array([x['user_selections'][stage] for x in instances])\n",
    "    true_positives = np.sum(np.logical_and(preds == 0, ground_truths == 0))\n",
    "    false_positives = np.sum(np.logical_and(preds == 0, ground_truths == 1))\n",
    "    true_negatives = np.sum(np.logical_and(preds == 1, ground_truths == 1))\n",
    "    false_negatives = np.sum(np.logical_and(preds == 1, ground_truths == 0))\n",
    "\n",
    "    unsure_rate = np.mean(preds == 2)\n",
    "    accuracy = (true_positives + true_negatives) / (true_positives + false_positives + true_negatives + false_negatives)\n",
    "    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives > 0 else 0\n",
    "    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0\n",
    "    false_positive_rate = false_positives / (false_positives + true_negatives) if false_positives + true_negatives > 0 else 0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if precision + recall > 0 else 0\n",
    "\n",
    "    # Utility: 0 if user is unsure, 1 if user correctly predicts AI correctness, -1 if user incorrectly predicts AI correctness\n",
    "    individual_utilities = np.array([0 if preds[i] == 2 else 1-2*np.abs(preds[i] - ground_truths[i]) for i in range(len(preds))])\n",
    "    utility = np.mean(individual_utilities)\n",
    "\n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'false_positive_rate': false_positive_rate,\n",
    "        'f1': f1,\n",
    "        'unsure_rate': unsure_rate,\n",
    "        'utility': utility,\n",
    "        'preds': preds,\n",
    "        'ground_truths': ground_truths,\n",
    "        'individual_utilities': individual_utilities,\n",
    "    }\n",
    "\n",
    "answeronly_results = evaluate_answers('answeronly', all_instances)\n",
    "withexplanation_results = evaluate_answers('withexplanation', all_instances)\n",
    "withexplanationquality_results = evaluate_answers('withexplanationquality', all_instances)\n",
    "\n",
    "print(\"Stage                   \\tUnsure Rate\\tAccuracy\\tPrecision\\tRecall\\t\\tF1\\t\\tFPR\\t\\tUtility\")\n",
    "print(\"-\"*140)\n",
    "for stage, results in zip(\n",
    "    ['Answer Only', 'With Explanation', 'With Explanation + Quality'], \n",
    "    [answeronly_results, withexplanation_results, withexplanationquality_results]\n",
    "):\n",
    "    print(f\"{stage:<25}\\t{results['unsure_rate']:.1%}\\t\\t{results['accuracy']:.1%}\\t\\t{results['precision']:.3f}\\t\\t{results['recall']:.3f}\\t\\t{results['f1']:.3f}\\t\\t{results['false_positive_rate']:.3f}\\t\\t{results['utility']:.3f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.21 ('friction')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1c1db964662c13cfe495ed1e8b69aeb7d6655dd637f8c5275c81541cdde6ef7d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
