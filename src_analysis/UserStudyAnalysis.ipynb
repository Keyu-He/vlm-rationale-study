{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting: showbothmetrics_llava1.5_with_image_q20_i10_s0\n",
      "30 sessions loaded from 10 files\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import glob, os\n",
    "\n",
    "USER_STUDIES_DIR = \"/home/shared/vlm_rationales_eval/user_studies_data/\"\n",
    "\n",
    "setting = 'showbothmetrics_llava1.5_with_image_q20_i10_s0'\n",
    "# setting = 'prodmetric_llava1.5_with_image_q20_i10_s0'\n",
    "print(f\"Setting: {setting}\")\n",
    "files = glob.glob(f'{USER_STUDIES_DIR}/prolific_batches/batch_interaction_data/{setting}/*.json')\n",
    "data = {}\n",
    "for file in files:\n",
    "    with open(file) as f:\n",
    "        data.update(json.load(f))\n",
    "print(f\"{len(data)} sessions loaded from {len(files)} files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_instances = sum([data[uid]['interactions'] for uid in data], [])\n",
    "len(all_instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage                   \tUnsure Rate\tTotalAcc\tNotUnsureAcc\tPrecision\tRecall\t\tF1\tFPR\t\tUtility\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Answer Only              \t70.3% ± 5.2%\t17.0% ± 4.3%\t57.3% ± 10.3%\t57.5% ± 11.3%\t85.7% ± 9.8%\t0.689\t77.5% ± 12.9%\t0.043 ± 0.062\n",
      "With Explanation         \t31.3% ± 5.2%\t38.0% ± 5.5%\t55.3% ± 6.8%\t55.8% ± 7.4%\t85.7% ± 6.5%\t0.676\t80.9% ± 8.0%\t0.073 ± 0.094\n",
      "With Explanation + Quality\t10.7% ± 3.5%\t57.3% ± 5.6%\t64.2% ± 5.7%\t61.4% ± 6.6%\t89.6% ± 5.0%\t0.729\t65.3% ± 8.4%\t0.253 ± 0.103\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the interactions\n",
    "from collections import Counter\n",
    "import math\n",
    "\n",
    "def compute_margin(p, n):\n",
    "    return 1.96*math.sqrt(p*(1-p)/n) if n > 0 else 0\n",
    "\n",
    "def compute_proportion_ci(p, n):\n",
    "    margin = compute_margin(p, n)\n",
    "    return (p - margin, p + margin)\n",
    "\n",
    "def compute_mean_ci(values):\n",
    "    n = len(values)\n",
    "    mean_val = np.mean(values)\n",
    "    std_val = np.std(values, ddof=1)\n",
    "    se = std_val/np.sqrt(n) if n > 0 else 0\n",
    "    return (mean_val - 1.96*se, mean_val + 1.96*se)\n",
    "\n",
    "def evaluate_answers(stage, instances):\n",
    "    ground_truths = np.array([1-x['question']['prediction_is_correct'] for x in instances])       # 0 means AI is correct, 1 means AI is incorrect\n",
    "    preds = np.array([x['user_selections'][stage] for x in instances])\n",
    "    true_positives = np.sum(np.logical_and(preds == 0, ground_truths == 0))\n",
    "    false_positives = np.sum(np.logical_and(preds == 0, ground_truths == 1))\n",
    "    true_negatives = np.sum(np.logical_and(preds == 1, ground_truths == 1))\n",
    "    false_negatives = np.sum(np.logical_and(preds == 1, ground_truths == 0))\n",
    "\n",
    "    unsure_rate = np.mean(preds == 2)\n",
    "    accuracy = (true_positives + true_negatives) / (true_positives + false_positives + true_negatives + false_negatives)\n",
    "    total_accuracy = (true_positives + true_negatives) / len(ground_truths)\n",
    "    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives > 0 else 0\n",
    "    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0\n",
    "    false_positive_rate = false_positives / (false_positives + true_negatives) if false_positives + true_negatives > 0 else 0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if precision + recall > 0 else 0\n",
    "\n",
    "    # Utility: 0 if user is unsure, 1 if user correctly predicts AI correctness, -1 if user incorrectly predicts AI correctness\n",
    "    individual_utilities = np.array([0 if preds[i] == 2 else 1-2*np.abs(preds[i] - ground_truths[i]) for i in range(len(preds))])\n",
    "    utility = np.mean(individual_utilities)\n",
    "    \n",
    "    # Compute 95% CIs.\n",
    "    total_accuracy_ci = compute_proportion_ci(total_accuracy, len(ground_truths))\n",
    "    unsure_rate_ci    = compute_proportion_ci(unsure_rate, len(preds))\n",
    "    \n",
    "    non_unsure = preds != 2\n",
    "    non_unsure_n = np.sum(non_unsure)\n",
    "    accuracy_ci = compute_proportion_ci(accuracy, non_unsure_n) if non_unsure_n > 0 else (0, 0)\n",
    "    \n",
    "    precision_n = true_positives + false_positives\n",
    "    precision_ci = compute_proportion_ci(precision, precision_n) if precision_n > 0 else (0, 0)\n",
    "    \n",
    "    recall_n = true_positives + false_negatives\n",
    "    recall_ci = compute_proportion_ci(recall, recall_n) if recall_n > 0 else (0, 0)\n",
    "    \n",
    "    fpr_n = false_positives + true_negatives\n",
    "    fpr_ci = compute_proportion_ci(false_positive_rate, fpr_n) if fpr_n > 0 else (0, 0)\n",
    "    \n",
    "    utility_ci = compute_mean_ci(individual_utilities)\n",
    "\n",
    "    return {\n",
    "        'total_accuracy': total_accuracy,\n",
    "        'total_accuracy_ci': total_accuracy_ci,\n",
    "        'accuracy': accuracy,\n",
    "        'accuracy_ci': accuracy_ci,\n",
    "        'precision': precision,\n",
    "        'precision_ci': precision_ci,\n",
    "        'recall': recall,\n",
    "        'recall_ci': recall_ci,\n",
    "        'false_positive_rate': false_positive_rate,\n",
    "        'fpr_ci': fpr_ci,\n",
    "        'f1': f1,\n",
    "        'unsure_rate': unsure_rate,\n",
    "        'unsure_rate_ci': unsure_rate_ci,\n",
    "        'utility': utility,\n",
    "        'utility_ci': utility_ci,\n",
    "        'preds': preds,\n",
    "        'ground_truths': ground_truths,\n",
    "        'individual_utilities': individual_utilities,\n",
    "    }\n",
    "\n",
    "answeronly_results = evaluate_answers('answeronly', all_instances)\n",
    "withexplanation_results = evaluate_answers('withexplanation', all_instances)\n",
    "withexplanationquality_results = evaluate_answers('withexplanationquality', all_instances)\n",
    "\n",
    "# print(\"Stage                   \\tUnsure Rate\\tTotalAcc\\tNotUnsureAcc\\tPrecision\\tRecall\\t\\tF1\\t\\tFPR\\t\\tUtility\")\n",
    "# print(\"-\"*140)\n",
    "# for stage, results in zip(\n",
    "#     ['Answer Only', 'With Explanation', 'With Explanation + Quality'], \n",
    "#     [answeronly_results, withexplanation_results, withexplanationquality_results]\n",
    "# ):\n",
    "#     print(f\"{stage:<25}\\t{results['unsure_rate']:.1%}\\t\\t{results['total_accuracy']:.1%}\\t\\t{results['accuracy']:.1%}\\t\\t{results['precision']:.3f}\\t\\t{results['recall']:.3f}\\t\\t{results['f1']:.3f}\\t\\t{results['false_positive_rate']:.3f}\\t\\t{results['utility']:.3f}\")\n",
    "    \n",
    "print(\"Stage                   \\tUnsure Rate\\tTotalAcc\\tNotUnsureAcc\\tPrecision\\tRecall\\t\\tF1\\tFPR\\t\\tUtility\")\n",
    "print(\"-\"*150) \n",
    "for stage_name, results in zip(\n",
    "    ['Answer Only', 'With Explanation', 'With Explanation + Quality'], \n",
    "    [answeronly_results, withexplanation_results, withexplanationquality_results]\n",
    "):\n",
    "    # Calculate margins as half the width of each CI.\n",
    "    unsure_margin    = (results['unsure_rate_ci'][1] - results['unsure_rate_ci'][0]) / 2\n",
    "    total_acc_margin = (results['total_accuracy_ci'][1] - results['total_accuracy_ci'][0]) / 2\n",
    "    acc_margin       = (results['accuracy_ci'][1] - results['accuracy_ci'][0]) / 2\n",
    "    prec_margin      = (results['precision_ci'][1] - results['precision_ci'][0]) / 2 if results['precision_ci'] != (0, 0) else 0\n",
    "    recall_margin    = (results['recall_ci'][1] - results['recall_ci'][0]) / 2 if results['recall_ci'] != (0, 0) else 0\n",
    "    fpr_margin       = (results['fpr_ci'][1] - results['fpr_ci'][0]) / 2 if results['fpr_ci'] != (0, 0) else 0\n",
    "    utility_margin   = (results['utility_ci'][1] - results['utility_ci'][0]) / 2\n",
    "\n",
    "    # For proportion metrics, we print as percentages; f1 is printed as a decimal (CI not computed); utility remains as a number.\n",
    "    print(f\"{stage_name:<25}\\t\"\n",
    "          f\"{results['unsure_rate']:.1%} ± {unsure_margin:.1%}\\t\"\n",
    "          f\"{results['total_accuracy']:.1%} ± {total_acc_margin:.1%}\\t\"\n",
    "          f\"{results['accuracy']:.1%} ± {acc_margin:.1%}\\t\"\n",
    "          f\"{results['precision']:.1%} ± {prec_margin:.1%}\\t\"\n",
    "          f\"{results['recall']:.1%} ± {recall_margin:.1%}\\t\"\n",
    "          f\"{results['f1']:.3f}\\t\"\n",
    "          f\"{results['false_positive_rate']:.1%} ± {fpr_margin:.1%}\\t\"\n",
    "          f\"{results['utility']:.3f} ± {utility_margin:.3f}\")\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
