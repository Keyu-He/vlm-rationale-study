{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Three Steps Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import glob, os\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# ---------------------------\n",
    "# Helper Functions\n",
    "# ---------------------------\n",
    "def compute_se(series):\n",
    "    \"\"\"Compute the standard error on a pandas Series.\"\"\"\n",
    "    n = series.count()\n",
    "    return series.std(ddof=1) / np.sqrt(n) if n > 1 else 0\n",
    "\n",
    "def compute_metrics(df, stage):\n",
    "    \"\"\"\n",
    "    Given a DataFrame of interactions (with a column for the stage and a column for ground_truth),\n",
    "    compute:\n",
    "      - Total accuracy (unsure responses, coded as 2, count as incorrect)\n",
    "      - Accuracy on non-unsure responses (\"NotUnsureAcc\")\n",
    "      - Unsure rate\n",
    "      - Utility (0 if unsure; 1 if correct; -1 if incorrect)\n",
    "    Returns also the computed utility as a Series.\n",
    "    \"\"\"\n",
    "    preds = df[stage]\n",
    "    # Total accuracy: count unsure as incorrect.\n",
    "    correct_all = ((preds != 2) & (preds == df['ground_truth'])).astype(float)\n",
    "    total_acc_mean = correct_all.mean()\n",
    "    total_acc_se = compute_se(correct_all)\n",
    "    \n",
    "    # Accuracy on non-unsure responses.\n",
    "    non_unsure = preds != 2\n",
    "    if non_unsure.sum() > 0:\n",
    "        correct_non_unsure = (df.loc[non_unsure, stage] == df.loc[non_unsure, 'ground_truth']).astype(float)\n",
    "        acc_mean = correct_non_unsure.mean()\n",
    "        acc_se = compute_se(correct_non_unsure)\n",
    "    else:\n",
    "        acc_mean, acc_se = np.nan, np.nan\n",
    "\n",
    "    # Unsure rate.\n",
    "    unsure_indicator = (preds == 2).astype(float)\n",
    "    unsure_mean = unsure_indicator.mean()\n",
    "    unsure_se = compute_se(unsure_indicator)\n",
    "    \n",
    "    # Utility: 0 if unsure; else 1 if correct, -1 if incorrect.\n",
    "    # (i.e., 1 - 2*abs(pred - ground_truth))\n",
    "    def calc_util(row):\n",
    "        p = row[stage]\n",
    "        gt = row['ground_truth']\n",
    "        return 0 if p == 2 else 1 - 2*abs(p - gt)\n",
    "    utility = df.apply(calc_util, axis=1)\n",
    "    util_mean = utility.mean()\n",
    "    util_se = compute_se(utility)\n",
    "    \n",
    "    return total_acc_mean, total_acc_se, acc_mean, acc_se, unsure_mean, unsure_se, util_mean, util_se, utility\n",
    "\n",
    "# ---------------------------\n",
    "# Settings and Directories\n",
    "# ---------------------------\n",
    "settings = [\n",
    "    'vf_numeric_llava1.5_with_image_q20_i10_s0',\n",
    "    'contr_numeric_llava1.5_with_image_q20_i10_s0',\n",
    "    'vf_descriptive_llava1.5_with_image_q20_i10_s0',\n",
    "    'contr_descriptive_llava1.5_with_image_q20_i10_s0',\n",
    "    'avg_vf_contr_numeric_llava1.5_with_image_q20_i10_s0',\n",
    "    'showbothmetrics_llava1.5_with_image_q20_i10_s0',\n",
    "    'showbothmetrics_descriptive_llava1.5_with_image_q20_i10_s0',\n",
    "    'vf_numeric_qwen2.5_vizwiz_q10_i10_s0',\n",
    "    'vf_descriptive_qwen2.5_vizwiz_q10_i10_s0',\n",
    "    'combined12_llava1.5_with_image_q20_i10_s0',\n",
    "    'combined123_llava1.5_with_image_q20_i10_s0',\n",
    "]\n",
    "USER_STUDIES_DIR = \"/home/shared/vlm_rationales_eval/user_studies_data/\"\n",
    "\n",
    "# To store scores and confusion matrices across settings\n",
    "all_setting_scores = []\n",
    "all_confusion_matrices = []\n",
    "consistent_choice_threshold = 0.9  # 90% threshold\n",
    "df_map = {}\n",
    "\n",
    "# ---------------------------\n",
    "# Process Each Setting via DataFrame\n",
    "# ---------------------------\n",
    "for setting in settings:\n",
    "    print(f\"\\nProcessing setting: {setting}\")\n",
    "    pattern = os.path.join(USER_STUDIES_DIR, \"prolific_batches\", \"batch_interaction_data\", setting, \"*.json\")\n",
    "    files = glob.glob(pattern)\n",
    "    \n",
    "    # Create a list of records (each record is an interaction)\n",
    "    records = []\n",
    "    for file in files:\n",
    "        batch_id = os.path.basename(file).split('.')[0]\n",
    "        with open(file) as f:\n",
    "            batch_data = json.load(f)\n",
    "        # Each file contains sessions for multiple users.\n",
    "        for user_id, session in batch_data.items():\n",
    "            interactions = session['interactions']\n",
    "            for interaction in interactions:\n",
    "                rec = {\n",
    "                    'user_id': user_id,\n",
    "                    'batch': batch_id,\n",
    "                    'question_i': interaction['question_i'],\n",
    "                    'answeronly': interaction['user_selections'].get('answeronly'),\n",
    "                    'withexplanation': interaction['user_selections'].get('withexplanation'),\n",
    "                    'withexplanationquality': interaction['user_selections'].get('withexplanationquality'),\n",
    "                    # Ground truth: defined as 1 - prediction_is_correct.\n",
    "                    'ground_truth': 1 - interaction['question']['prediction_is_correct']\n",
    "                }\n",
    "                records.append(rec)\n",
    "    \n",
    "    # Build DataFrame from the records.\n",
    "    df = pd.DataFrame(records)\n",
    "    print(f\"Found {df['user_id'].nunique()} users and {len(df)} interactions for setting '{setting}'.\")\n",
    "    \n",
    "    # ---------------------------\n",
    "    # Filter Users Based on Consistency\n",
    "    # ---------------------------\n",
    "    # For each user, look at all responses across all stages and drop users\n",
    "    # where one response (0, 1, or 2) comprises ≥90% of their responses.\n",
    "    def user_is_valid(group):\n",
    "        responses = pd.concat([group['answeronly'], group['withexplanation'], group['withexplanationquality']])\n",
    "        if responses.empty:\n",
    "            return False\n",
    "        norm_counts = responses.value_counts(normalize=True)\n",
    "        if norm_counts.max() >= consistent_choice_threshold:\n",
    "            # If desired, you could print which user is filtered out.\n",
    "            # print(f\"User {group.name} filtered out; response distribution: {norm_counts.to_dict()}\")\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    df_valid = df.groupby('user_id').filter(user_is_valid)\n",
    "\n",
    "    if df_valid.empty:\n",
    "        print(\"No interactions remain after filtering. Skipping evaluation for this setting.\")\n",
    "        continue\n",
    "        \n",
    "    # ---------------------------\n",
    "    # Filter Batches with More Than 30 Annotations\n",
    "    # ---------------------------\n",
    "    # For each batch, if it has been annotated more than 30 times (i.e. more than three users,\n",
    "    # assuming each user annotates about 10 interactions), print the annotation count\n",
    "    # and then shorten the batch to include only the first 30 annotations.\n",
    "    batch_counts = df_valid['batch'].value_counts()\n",
    "    for batch, count in batch_counts.items():\n",
    "        if count > 30:\n",
    "            print(f\"Batch {batch} has been annotated {count} times, exceeding 30 annotations. Shortening to first 30 annotations.\")\n",
    "    # Group by 'batch' and take only the first 30 entries for each batch\n",
    "    df_valid = df_valid.groupby('batch').apply(lambda g: g.head(30)).reset_index(drop=True)\n",
    "    print(f\"{df_valid['user_id'].nunique()} users remain after filtering; {len(df_valid)} interactions remain.\")\n",
    "    \n",
    "    \n",
    "    # Indicate which batches need to add users\n",
    "    batch_counts = df_valid['batch'].value_counts()\n",
    "    for batch, count in batch_counts.items():\n",
    "        if count % 10: raise ValueError('# Annotation should be divisible by 10, but found not??')\n",
    "        count = count // 10\n",
    "        if count > 3:\n",
    "            raise ValueError(f\"After filtering, # users taking the batch should not > 3??\")\n",
    "        diff = 3 - count\n",
    "        if diff != 0:\n",
    "            print(f\"Batch {batch} need {diff} more users to annotate.\")\n",
    "            \n",
    "    # Add df to the df_map for later use.\n",
    "    df_map[setting] = df_valid\n",
    "    \n",
    "    \n",
    "    # ---------------------------\n",
    "    # Compute Metrics for Each Stage Using DataFrames\n",
    "    # ---------------------------\n",
    "    stage_names = ['Answer Only', 'With Explanation', 'With Explanation + Quality']\n",
    "    stages = ['answeronly', 'withexplanation', 'withexplanationquality']\n",
    "    metrics = {}\n",
    "    utilities = {}\n",
    "    csv_prints = []\n",
    "    print(\"\\nStage                    \\tUnsure Rate\\tTotalAcc\\tNotUnsureAcc\\tUtility\")\n",
    "    for stage, name in zip(stages, stage_names):\n",
    "        tot_acc, tot_se, acc_mean, acc_se, unsure_mean, unsure_se, util_mean, util_se, util_series = compute_metrics(df_valid, stage)\n",
    "        metrics[stage] = {\n",
    "            'total_accuracy_mean': tot_acc,\n",
    "            'total_accuracy_se': tot_se,\n",
    "            'accuracy_mean': acc_mean,\n",
    "            'accuracy_se': acc_se,\n",
    "            'unsure_rate_mean': unsure_mean,\n",
    "            'unsure_rate_se': unsure_se,\n",
    "            'utility_mean': util_mean,\n",
    "            'utility_se': util_se\n",
    "        }\n",
    "        utilities[stage] = util_series\n",
    "        print(f\"{name:<25}\\t{unsure_mean*100:5.1f}% ± {unsure_se*100:4.1f}%\\t\"\n",
    "              f\"{tot_acc*100:5.1f}% ± {tot_se*100:4.1f}%\\t\"\n",
    "              f\"{acc_mean*100:5.1f}% ± {acc_se*100:4.1f}%\\t\"\n",
    "              f\"{util_mean:+.3f} ± {util_se:.3f}\")\n",
    "        csv_prints.append(f\"{unsure_mean*100:5.1f}% ± {unsure_se*100:4.1f}%, \"\n",
    "              f\"{tot_acc*100:5.1f}% ± {tot_se*100:4.1f}%, \"\n",
    "              f\"{acc_mean*100:5.1f}% ± {acc_se*100:4.1f}%, \"\n",
    "              f\"{util_mean:+.3f} ± {util_se:.3f}\")\n",
    "        \n",
    "    print(\", \".join(csv_prints))\n",
    "\n",
    "    # ---------------------------\n",
    "    # Utility Gains Between Stages\n",
    "    # ---------------------------\n",
    "    gain_explanation = utilities['withexplanation'] - utilities['answeronly']\n",
    "    gain_quality = utilities['withexplanationquality'] - utilities['withexplanation']\n",
    "    expl_gain_mean = gain_explanation.mean()\n",
    "    expl_gain_se = gain_explanation.std(ddof=1) / np.sqrt(len(gain_explanation)) if len(gain_explanation) > 1 else 0\n",
    "    qual_gain_mean = gain_quality.mean()\n",
    "    qual_gain_se = gain_quality.std(ddof=1) / np.sqrt(len(gain_quality)) if len(gain_quality) > 1 else 0\n",
    "    \n",
    "    print(f\"\\nSetting: {setting}\")\n",
    "    print(\"Stage\\t\\tUnsure Rate\\tUtility over previous stage\")\n",
    "    print(f\"Explanation\\t {metrics['withexplanation']['unsure_rate_mean']*100:5.1f}%\\t\\t {expl_gain_mean:+.3f} ± {expl_gain_se:.3f}\")\n",
    "    print(f\"Quality\\t\\t {metrics['withexplanationquality']['unsure_rate_mean']*100:5.1f}%\\t\\t {qual_gain_mean:+.3f} ± {qual_gain_se:.3f}\")\n",
    "    \n",
    "    # ---------------------------\n",
    "    # Compute Confusion Matrix and M1–M4 Scores\n",
    "    # ---------------------------\n",
    "    # Compute utility for withexplanation and withexplanationquality directly.\n",
    "    def compute_utility(val, gt):\n",
    "        return 0 if val == 2 else 1 - 2*abs(val-gt)\n",
    "    \n",
    "    df_valid['util_withexplanation'] = df_valid.apply(lambda row: compute_utility(row['withexplanation'], row['ground_truth']), axis=1)\n",
    "    df_valid['util_withexplanationquality'] = df_valid.apply(lambda row: compute_utility(row['withexplanationquality'], row['ground_truth']), axis=1)\n",
    "    \n",
    "    # For the confusion matrix, map utilities to indices: -1 -> 0, 0 -> 1, 1 -> 2.\n",
    "    mapping = {-1: 0, 0: 1, 1: 2}\n",
    "    label_order = [-1, 0, 1]\n",
    "    label_names = ['Incorrect', 'Unsure', 'Correct']\n",
    "    \n",
    "    try:\n",
    "        cm = np.zeros((3, 3), dtype=int)\n",
    "        for _, row in df_valid.iterrows():\n",
    "            u2 = row['util_withexplanation']\n",
    "            u3 = row['util_withexplanationquality']\n",
    "            cm[mapping[u2], mapping[u3]] += 1\n",
    "        cm_transposed = cm.T\n",
    "    except Exception as e:\n",
    "        print(f\"Error computing confusion matrix or M1–M4 scores: {e}\")\n",
    "        print(f\"This would be expected if setting {setting} is combined12 or combined123.\")\n",
    "        continue\n",
    "        \n",
    "    print(f\"\\nSetting: {setting} Step 2 -> Step 3 (transposed)\")\n",
    "    print(\"\\t\\t\" + \"\\t\".join(label_names))\n",
    "    for i, label in enumerate(label_names):\n",
    "        row_values = \"\\t\".join(str(cm_transposed[i, j]) for j in range(3))\n",
    "        print(f\"{label:<10}\\t {row_values}\")\n",
    "    \n",
    "    # Compute M1–M4 using counts from the confusion matrix.\n",
    "    def count_transitions(val2, val3):\n",
    "        return ((df_valid['util_withexplanation'] == val2) & (df_valid['util_withexplanationquality'] == val3)).sum()\n",
    "    \n",
    "    denom_M1 = count_transitions(0, -1) + count_transitions(0, 0) + count_transitions(0, 1)\n",
    "    num_M1 = count_transitions(0, -1) + count_transitions(0, 1)\n",
    "    m1 = num_M1 / denom_M1 if denom_M1 > 0 else np.nan\n",
    "\n",
    "    denom_M2 = count_transitions(0, -1) + count_transitions(0, 1)\n",
    "    m2 = count_transitions(0, 1) / denom_M2 if denom_M2 > 0 else np.nan\n",
    "\n",
    "    denom_M3 = count_transitions(-1, -1) + count_transitions(-1, 0) + count_transitions(-1, 1)\n",
    "    num_M3 = count_transitions(-1, 0) + count_transitions(-1, 1)\n",
    "    m3 = num_M3 / denom_M3 if denom_M3 > 0 else np.nan\n",
    "\n",
    "    denom_M4 = count_transitions(1, -1) + count_transitions(1, 0) + count_transitions(1, 1)\n",
    "    num_M4 = count_transitions(1, -1) + count_transitions(1, 0)\n",
    "    m4 = num_M4 / denom_M4 if denom_M4 > 0 else np.nan\n",
    "\n",
    "    print(\"\\nM1–M4 Scores:\")\n",
    "    print(f\"M1 (P(S3 ≠ U | S2 = U)): {m1:.2%}\")\n",
    "    print(f\"M2 (P(S3 = C | S2 = U, S3 ≠ U)): {m2:.2%}\")\n",
    "    print(f\"M3 (P(S3 ≠ I | S2 = I)): {m3:.2%}\")\n",
    "    print(f\"M4 (P(S3 ≠ C | S2 = C)): {m4:.2%}\")\n",
    "    print(f\"{m1:.2%}, {m2:.2%}, {m3:.2%}, {m4:.2%}, {m3-m4:.2%}\")\n",
    "\n",
    "    \n",
    "    # ---------------------------\n",
    "    # Print Examples Where Step 2 is Correct and Step 3 is Incorrect\n",
    "    # ---------------------------\n",
    "    print(\"\\nExamples where Step 2 is correct and Step 3 is incorrect:\")\n",
    "    examples = df_valid[\n",
    "        (df_valid['withexplanation'] != 2) &\n",
    "        (df_valid['withexplanation'] == df_valid['ground_truth']) &\n",
    "        (df_valid['withexplanationquality'] != 2) &\n",
    "        (df_valid['withexplanationquality'] != df_valid['ground_truth'])\n",
    "    ]\n",
    "    if not examples.empty:\n",
    "        for idx, row in examples.head(5).iterrows():\n",
    "            print(row.to_json())\n",
    "    else:\n",
    "        print(\"No examples found.\")\n",
    "    \n",
    "    # ---------------------------\n",
    "    # Plot: Confusion Matrix and M1–M4 Scores for the Current Setting\n",
    "    # ---------------------------\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12, 5))\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    ax = axes[0]\n",
    "    im = ax.imshow(cm_transposed, cmap='Greens', interpolation='nearest')\n",
    "    ax.set_title(f\"Setting {setting.split('_llava')[0]} Step 2->Step 3\\n(WithExplanation -> WithExplanationQuality)\")\n",
    "    ax.set_xticks(np.arange(len(label_names)))\n",
    "    ax.set_yticks(np.arange(len(label_names)))\n",
    "    ax.set_xticklabels(label_names)\n",
    "    ax.set_yticklabels(label_names)\n",
    "    ax.set_xlabel(\"WithExplanation\")\n",
    "    ax.set_ylabel(\"WithExplanationQuality\")\n",
    "    for i in range(len(label_names)):\n",
    "        for j in range(len(label_names)):\n",
    "            ax.text(j, i, cm_transposed[i, j], ha=\"center\", va=\"center\", color=\"black\")\n",
    "    \n",
    "    # Plot M1–M4 scores as text.\n",
    "    ax = axes[1]\n",
    "    ax.axis('off')\n",
    "    score_text = (f\"M1 (P(S3 ≠ U | S2 = U)): {m1:.2%}\\n\"\n",
    "                  f\"M2 (P(S3 = C | S2 = U, S3 ≠ U)): {m2:.2%}\\n\"\n",
    "                  f\"M3 (P(S3 ≠ I | S2 = I)): {m3:.2%}\\n\"\n",
    "                  f\"M4 (P(S3 ≠ C | S2 = C)): {m4:.2%}\")\n",
    "    ax.text(0.5, 0.5, score_text, fontsize=12, ha='center', va='center', transform=ax.transAxes)\n",
    "    ax.set_title(\"M1–M4 Scores\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Save scores and confusion matrix for later comparison/plotting.\n",
    "    all_setting_scores.append({\n",
    "        \"Setting\": setting,\n",
    "        \"Resolution Rate\": m1,\n",
    "        \"Positive Conversion Rate\": m2,\n",
    "        \"Error Recovery Rate\": m3,\n",
    "        \"Correct Loss Rate\": m4\n",
    "    })\n",
    "    all_confusion_matrices.append((setting, cm_transposed))\n",
    "\n",
    "# ---------------------------\n",
    "# Plot Grouped Bar Chart for M1–M4 Scores Across Settings\n",
    "# ---------------------------\n",
    "if all_setting_scores:\n",
    "    df_scores = pd.DataFrame(all_setting_scores)\n",
    "    df_scores[\"SettingShort\"] = df_scores[\"Setting\"].apply(lambda s: s.split('_llava')[0])\n",
    "    df_scores[\"Recovery Gap\"] = df_scores[\"Error Recovery Rate\"] - df_scores[\"Correct Loss Rate\"]\n",
    "    df_scores = df_scores.set_index(\"SettingShort\")\n",
    "    \n",
    "    # Transpose so that rows become score types.\n",
    "    df_scores_T = df_scores[[\"Resolution Rate\", \"Positive Conversion Rate\", \"Error Recovery Rate\",\n",
    "                             \"Correct Loss Rate\", \"Recovery Gap\"]].transpose()\n",
    "    \n",
    "    ax = df_scores_T.plot(kind='bar', figsize=(12, 6))\n",
    "    ax.set_ylabel(\"Score (Fraction)\")\n",
    "    ax.set_title(\"Scores by Score Type Across Settings\")\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), rotation=45)\n",
    "    ax.legend(title=\"Setting\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# ---------------------------\n",
    "# Plot a Grid of Confusion Matrices for Every Setting\n",
    "# ---------------------------\n",
    "if all_confusion_matrices:\n",
    "    n_settings = len(all_confusion_matrices)\n",
    "    ncols = 2\n",
    "    nrows = math.ceil(n_settings / ncols)\n",
    "    fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(ncols*5, nrows*5))\n",
    "    if n_settings > 1:\n",
    "        axes = np.array(axes).flatten()\n",
    "    else:\n",
    "        axes = [axes]\n",
    "        \n",
    "    for ax, (setting, cm_transposed) in zip(axes, all_confusion_matrices):\n",
    "        im = ax.imshow(cm_transposed, cmap='Greens', interpolation='nearest')\n",
    "        ax.set_title(setting.split('_llava')[0])\n",
    "        ax.set_xticks(np.arange(len(label_names)))\n",
    "        ax.set_yticks(np.arange(len(label_names)))\n",
    "        ax.set_xticklabels(label_names)\n",
    "        ax.set_yticklabels(label_names)\n",
    "        ax.set_xlabel(\"WithExplanation\")\n",
    "        ax.set_ylabel(\"WithExplanationQuality\")\n",
    "        for i in range(len(label_names)):\n",
    "            for j in range(len(label_names)):\n",
    "                ax.text(j, i, cm_transposed[i, j], ha=\"center\", va=\"center\", color=\"black\")\n",
    "                \n",
    "    # Turn off any extra axes.\n",
    "    for ax in axes[len(all_confusion_matrices):]:\n",
    "        ax.axis('off')\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# Updated Bar Plot for Resolution Rate and Error Recovery Rate (Separate Plots)\n",
    "# with Larger Text, Renamed x‑Tick Labels, and Specified Y‑Range for Resolution Rate Plot\n",
    "# ---------------------------\n",
    "if all_setting_scores:\n",
    "    import matplotlib.ticker as mtick\n",
    "    # Create a DataFrame from the scores and simplify the setting names.\n",
    "    df_scores = pd.DataFrame(all_setting_scores)\n",
    "    df_scores[\"SettingShort\"] = df_scores[\"Setting\"].apply(lambda s: s.split('_llava')[0])\n",
    "    df_scores = df_scores.set_index(\"SettingShort\")\n",
    "    df_scores.index.name = None  # Remove the index label from the x-axis.\n",
    "    \n",
    "    # Define a mapping from the abbreviated setting names to more natural names.\n",
    "    rename_mapping = {\n",
    "        'vf_numeric': 'Numeric VF',\n",
    "        'contr_numeric': 'Numeric CONTR',\n",
    "        'showbothmetrics': 'Both Metrics (Numeric)',\n",
    "        'avg_vf_contr_numeric': 'Average VF/CONTR (Numeric)',\n",
    "        'showbothmetrics_descriptive': 'Both Metrics (Descriptive)',\n",
    "        'vf_descriptive': 'Descriptive VF',\n",
    "        'contr_descriptive': 'Descriptive CONTR'\n",
    "    }\n",
    "    \n",
    "    # Extract the two metrics separately.\n",
    "    resolution = df_scores[\"Resolution Rate\"]\n",
    "    error_recovery = df_scores[\"Error Recovery Rate\"]\n",
    "    \n",
    "    # Set up subplots for each metric.\n",
    "    fig, axes = plt.subplots(ncols=2, figsize=(12, 7))\n",
    "    \n",
    "    # Plot Resolution Rate.\n",
    "    resolution.plot(kind=\"bar\", ax=axes[0], color=\"skyblue\")\n",
    "    axes[0].set_title(\"Resolution Rate Across Settings\", fontsize=16, fontweight=\"bold\")\n",
    "    axes[0].set_ylabel(\"Resolution Rate (%)\", fontsize=16)\n",
    "    axes[0].grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "    axes[0].set_xlabel(\"\")  # Remove the x-axis label\n",
    "    axes[0].tick_params(axis=\"both\", labelsize=12)\n",
    "    # Replace x tick labels using the mapping.\n",
    "    new_labels = [rename_mapping.get(label.get_text(), label.get_text()) \n",
    "                  for label in axes[0].get_xticklabels()]\n",
    "    axes[0].set_xticklabels(new_labels, rotation=45, ha=\"right\", fontsize=16)\n",
    "    axes[0].yaxis.set_major_formatter(mtick.PercentFormatter(1.0))\n",
    "    # Set custom y-axis range\n",
    "    axes[0].set_ylim(0.6, 1)\n",
    "    # Annotate each bar (multiplying value by 100 for percentage display).\n",
    "    for rect in axes[0].patches:\n",
    "        height = rect.get_height()\n",
    "        axes[0].text(rect.get_x() + rect.get_width()/2, height, f\"{height*100:.1f}%\",\n",
    "                     ha=\"center\", va=\"bottom\", fontsize=14)\n",
    "    \n",
    "    # Plot Error Recovery Rate.\n",
    "    error_recovery.plot(kind=\"bar\", ax=axes[1], color=\"lightgreen\")\n",
    "    axes[1].set_title(\"Error Recovery Rate Across Settings\", fontsize=16, fontweight=\"bold\")\n",
    "    axes[1].set_ylabel(\"Error Recovery Rate (%)\", fontsize=16)\n",
    "    axes[1].grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "    axes[1].set_xlabel(\"\")  # Remove the x-axis label\n",
    "    axes[1].tick_params(axis=\"both\", labelsize=12)\n",
    "    new_labels = [rename_mapping.get(label.get_text(), label.get_text()) \n",
    "                  for label in axes[1].get_xticklabels()]\n",
    "    axes[1].set_xticklabels(new_labels, rotation=45, ha=\"right\", fontsize=16)\n",
    "    axes[1].yaxis.set_major_formatter(mtick.PercentFormatter(1.0))\n",
    "    axes[1].set_ylim(0, 0.35)\n",
    "    for rect in axes[1].patches:\n",
    "        height = rect.get_height()\n",
    "        axes[1].text(rect.get_x() + rect.get_width()/2, height, f\"{height*100:.1f}%\",\n",
    "                     ha=\"center\", va=\"bottom\", fontsize=14)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store df_map to a file for later use.\n",
    "import pickle\n",
    "for setting, df in df_map.items():\n",
    "    # only keep the columns we need\n",
    "    df_map[setting] = df[['user_id', 'batch', 'question_i', 'answeronly', 'withexplanation', 'withexplanationquality', 'ground_truth']]\n",
    "with open('df_map.pkl', 'wb') as f:\n",
    "    pickle.dump(df_map, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combined123 Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing setting: combined12_llava1.5_aokvqa_better_sampled_q10_i10_s0\n",
      "                      user_id       batch  question_i  answeronly  \\\n",
      "0    66b6cb18cc43c649a5917319  000_3users           0          -1   \n",
      "1    66b6cb18cc43c649a5917319  000_3users           1          -1   \n",
      "2    66b6cb18cc43c649a5917319  000_3users           2          -1   \n",
      "3    66b6cb18cc43c649a5917319  000_3users           3          -1   \n",
      "4    66b6cb18cc43c649a5917319  000_3users           4          -1   \n",
      "..                        ...         ...         ...         ...   \n",
      "295  663abe859bcc3ea8ef9bbdf3  009_3users           5          -1   \n",
      "296  663abe859bcc3ea8ef9bbdf3  009_3users           6          -1   \n",
      "297  663abe859bcc3ea8ef9bbdf3  009_3users           7          -1   \n",
      "298  663abe859bcc3ea8ef9bbdf3  009_3users           8          -1   \n",
      "299  663abe859bcc3ea8ef9bbdf3  009_3users           9          -1   \n",
      "\n",
      "     withexplanation  withexplanationquality  ground_truth  \n",
      "0                 -1                       0             0  \n",
      "1                 -1                       0             0  \n",
      "2                 -1                       0             0  \n",
      "3                 -1                       0             0  \n",
      "4                 -1                       0             1  \n",
      "..               ...                     ...           ...  \n",
      "295               -1                       0             1  \n",
      "296               -1                       0             0  \n",
      "297               -1                       1             0  \n",
      "298               -1                       2             0  \n",
      "299               -1                       1             1  \n",
      "\n",
      "[300 rows x 7 columns]\n",
      "Found 29 users and 300 interactions for setting 'combined12_llava1.5_aokvqa_better_sampled_q10_i10_s0'.\n",
      "29 users remain after filtering; 300 interactions remain.\n",
      "\n",
      "Stage                    \tUnsure Rate\tTotalAcc\tNotUnsureAcc\tUtility\n",
      "Answer Only              \t  0.0% ±  0.0%\t  0.0% ±  0.0%\t  0.0% ±  0.0%\t-2.000 ± 0.058\n",
      "With Explanation         \t  0.0% ±  0.0%\t  0.0% ±  0.0%\t  0.0% ±  0.0%\t-2.000 ± 0.058\n",
      "With Explanation + Quality\t  8.7% ±  1.6%\t 54.0% ±  2.9%\t 59.1% ±  3.0%\t+0.167 ± 0.054\n",
      "  0.0% ±  0.0%,   0.0% ±  0.0%,   0.0% ±  0.0%, -2.000 ± 0.058,   0.0% ±  0.0%,   0.0% ±  0.0%,   0.0% ±  0.0%, -2.000 ± 0.058,   8.7% ±  1.6%,  54.0% ±  2.9%,  59.1% ±  3.0%, +0.167 ± 0.054\n",
      "\n",
      "Setting: combined12_llava1.5_aokvqa_better_sampled_q10_i10_s0\n",
      "Stage\t\tUnsure Rate\tUtility over previous stage\n",
      "Explanation\t   0.0%\t\t +0.000 ± 0.000\n",
      "Quality\t\t   8.7%\t\t +2.167 ± 0.056\n",
      "Error computing confusion matrix or M1–M4 scores: -3\n",
      "This would be expected if setting combined12_llava1.5_aokvqa_better_sampled_q10_i10_s0 is combined12 or combined123.\n",
      "\n",
      "Processing setting: prodmetric_llava1.5_aokvqa_better_sampled_q10_i10_s0\n",
      "                      user_id       batch  question_i  answeronly  \\\n",
      "0    66d9fc2d2b46f520118ea65f  000_3users           0          -1   \n",
      "1    66d9fc2d2b46f520118ea65f  000_3users           1          -1   \n",
      "2    66d9fc2d2b46f520118ea65f  000_3users           2          -1   \n",
      "3    66d9fc2d2b46f520118ea65f  000_3users           3          -1   \n",
      "4    66d9fc2d2b46f520118ea65f  000_3users           4          -1   \n",
      "..                        ...         ...         ...         ...   \n",
      "295  66f3226fce0f907787b2e811  009_3users           5          -1   \n",
      "296  66f3226fce0f907787b2e811  009_3users           6          -1   \n",
      "297  66f3226fce0f907787b2e811  009_3users           7          -1   \n",
      "298  66f3226fce0f907787b2e811  009_3users           8          -1   \n",
      "299  66f3226fce0f907787b2e811  009_3users           9          -1   \n",
      "\n",
      "     withexplanation  withexplanationquality  ground_truth  \n",
      "0                 -1                       0             0  \n",
      "1                 -1                       0             0  \n",
      "2                 -1                       0             0  \n",
      "3                 -1                       0             0  \n",
      "4                 -1                       1             1  \n",
      "..               ...                     ...           ...  \n",
      "295               -1                       1             1  \n",
      "296               -1                       0             0  \n",
      "297               -1                       0             0  \n",
      "298               -1                       0             0  \n",
      "299               -1                       1             1  \n",
      "\n",
      "[300 rows x 7 columns]\n",
      "Found 30 users and 300 interactions for setting 'prodmetric_llava1.5_aokvqa_better_sampled_q10_i10_s0'.\n",
      "30 users remain after filtering; 300 interactions remain.\n",
      "\n",
      "Stage                    \tUnsure Rate\tTotalAcc\tNotUnsureAcc\tUtility\n",
      "Answer Only              \t  0.0% ±  0.0%\t  0.0% ±  0.0%\t  0.0% ±  0.0%\t-2.000 ± 0.058\n",
      "With Explanation         \t  0.0% ±  0.0%\t  0.0% ±  0.0%\t  0.0% ±  0.0%\t-2.000 ± 0.058\n",
      "With Explanation + Quality\t  9.3% ±  1.7%\t 63.7% ±  2.8%\t 70.2% ±  2.8%\t+0.367 ± 0.051\n",
      "  0.0% ±  0.0%,   0.0% ±  0.0%,   0.0% ±  0.0%, -2.000 ± 0.058,   0.0% ±  0.0%,   0.0% ±  0.0%,   0.0% ±  0.0%, -2.000 ± 0.058,   9.3% ±  1.7%,  63.7% ±  2.8%,  70.2% ±  2.8%, +0.367 ± 0.051\n",
      "\n",
      "Setting: prodmetric_llava1.5_aokvqa_better_sampled_q10_i10_s0\n",
      "Stage\t\tUnsure Rate\tUtility over previous stage\n",
      "Explanation\t   0.0%\t\t +0.000 ± 0.000\n",
      "Quality\t\t   9.3%\t\t +2.367 ± 0.059\n",
      "Error computing confusion matrix or M1–M4 scores: -3\n",
      "This would be expected if setting prodmetric_llava1.5_aokvqa_better_sampled_q10_i10_s0 is combined12 or combined123.\n",
      "\n",
      "Processing setting: vf_numeric_llava1.5_aokvqa_better_sampled_q10_i10_s0\n",
      "                      user_id       batch  question_i  answeronly  \\\n",
      "0    63483c4522d65d46028da7a7  000_3users           0          -1   \n",
      "1    63483c4522d65d46028da7a7  000_3users           1          -1   \n",
      "2    63483c4522d65d46028da7a7  000_3users           2          -1   \n",
      "3    63483c4522d65d46028da7a7  000_3users           3          -1   \n",
      "4    63483c4522d65d46028da7a7  000_3users           4          -1   \n",
      "..                        ...         ...         ...         ...   \n",
      "295  672a6ed8d37a4143e096929b  009_3users           5          -1   \n",
      "296  672a6ed8d37a4143e096929b  009_3users           6          -1   \n",
      "297  672a6ed8d37a4143e096929b  009_3users           7          -1   \n",
      "298  672a6ed8d37a4143e096929b  009_3users           8          -1   \n",
      "299  672a6ed8d37a4143e096929b  009_3users           9          -1   \n",
      "\n",
      "     withexplanation  withexplanationquality  ground_truth  \n",
      "0                 -1                       0             0  \n",
      "1                 -1                       0             0  \n",
      "2                 -1                       0             0  \n",
      "3                 -1                       0             0  \n",
      "4                 -1                       0             1  \n",
      "..               ...                     ...           ...  \n",
      "295               -1                       0             1  \n",
      "296               -1                       0             0  \n",
      "297               -1                       0             0  \n",
      "298               -1                       0             0  \n",
      "299               -1                       0             1  \n",
      "\n",
      "[300 rows x 7 columns]\n",
      "Found 30 users and 300 interactions for setting 'vf_numeric_llava1.5_aokvqa_better_sampled_q10_i10_s0'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_16134/604509645.py:143: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_valid = df_valid.groupby('batch').apply(lambda g: g.head(30)).reset_index(drop=True)\n",
      "/tmp/ipykernel_16134/604509645.py:143: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_valid = df_valid.groupby('batch').apply(lambda g: g.head(30)).reset_index(drop=True)\n",
      "/tmp/ipykernel_16134/604509645.py:143: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_valid = df_valid.groupby('batch').apply(lambda g: g.head(30)).reset_index(drop=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 users remain after filtering; 300 interactions remain.\n",
      "\n",
      "Stage                    \tUnsure Rate\tTotalAcc\tNotUnsureAcc\tUtility\n",
      "Answer Only              \t  0.0% ±  0.0%\t  0.0% ±  0.0%\t  0.0% ±  0.0%\t-2.000 ± 0.058\n",
      "With Explanation         \t  0.0% ±  0.0%\t  0.0% ±  0.0%\t  0.0% ±  0.0%\t-2.000 ± 0.058\n",
      "With Explanation + Quality\t  7.3% ±  1.5%\t 58.3% ±  2.9%\t 62.9% ±  2.9%\t+0.240 ± 0.054\n",
      "  0.0% ±  0.0%,   0.0% ±  0.0%,   0.0% ±  0.0%, -2.000 ± 0.058,   0.0% ±  0.0%,   0.0% ±  0.0%,   0.0% ±  0.0%, -2.000 ± 0.058,   7.3% ±  1.5%,  58.3% ±  2.9%,  62.9% ±  2.9%, +0.240 ± 0.054\n",
      "\n",
      "Setting: vf_numeric_llava1.5_aokvqa_better_sampled_q10_i10_s0\n",
      "Stage\t\tUnsure Rate\tUtility over previous stage\n",
      "Explanation\t   0.0%\t\t +0.000 ± 0.000\n",
      "Quality\t\t   7.3%\t\t +2.240 ± 0.055\n",
      "Error computing confusion matrix or M1–M4 scores: -3\n",
      "This would be expected if setting vf_numeric_llava1.5_aokvqa_better_sampled_q10_i10_s0 is combined12 or combined123.\n",
      "\n",
      "Processing setting: vf_descriptive_llava1.5_aokvqa_better_sampled_q10_i10_s0\n",
      "                      user_id       batch  question_i  answeronly  \\\n",
      "0    670348a0446814e5ead1e672  000_3users           0          -1   \n",
      "1    670348a0446814e5ead1e672  000_3users           1          -1   \n",
      "2    670348a0446814e5ead1e672  000_3users           2          -1   \n",
      "3    670348a0446814e5ead1e672  000_3users           3          -1   \n",
      "4    670348a0446814e5ead1e672  000_3users           4          -1   \n",
      "..                        ...         ...         ...         ...   \n",
      "145  6720378a683a5d8a933493fb  004_3users           5          -1   \n",
      "146  6720378a683a5d8a933493fb  004_3users           6          -1   \n",
      "147  6720378a683a5d8a933493fb  004_3users           7          -1   \n",
      "148  6720378a683a5d8a933493fb  004_3users           8          -1   \n",
      "149  6720378a683a5d8a933493fb  004_3users           9          -1   \n",
      "\n",
      "     withexplanation  withexplanationquality  ground_truth  \n",
      "0                 -1                       1             0  \n",
      "1                 -1                       1             0  \n",
      "2                 -1                       0             0  \n",
      "3                 -1                       0             0  \n",
      "4                 -1                       1             1  \n",
      "..               ...                     ...           ...  \n",
      "145               -1                       1             0  \n",
      "146               -1                       1             1  \n",
      "147               -1                       0             0  \n",
      "148               -1                       0             1  \n",
      "149               -1                       1             1  \n",
      "\n",
      "[150 rows x 7 columns]\n",
      "Found 15 users and 150 interactions for setting 'vf_descriptive_llava1.5_aokvqa_better_sampled_q10_i10_s0'.\n",
      "15 users remain after filtering; 150 interactions remain.\n",
      "\n",
      "Stage                    \tUnsure Rate\tTotalAcc\tNotUnsureAcc\tUtility\n",
      "Answer Only              \t  0.0% ±  0.0%\t  0.0% ±  0.0%\t  0.0% ±  0.0%\t-2.000 ± 0.082\n",
      "With Explanation         \t  0.0% ±  0.0%\t  0.0% ±  0.0%\t  0.0% ±  0.0%\t-2.000 ± 0.082\n",
      "With Explanation + Quality\t  5.3% ±  1.8%\t 60.7% ±  4.0%\t 64.1% ±  4.0%\t+0.267 ± 0.077\n",
      "  0.0% ±  0.0%,   0.0% ±  0.0%,   0.0% ±  0.0%, -2.000 ± 0.082,   0.0% ±  0.0%,   0.0% ±  0.0%,   0.0% ±  0.0%, -2.000 ± 0.082,   5.3% ±  1.8%,  60.7% ±  4.0%,  64.1% ±  4.0%, +0.267 ± 0.077\n",
      "\n",
      "Setting: vf_descriptive_llava1.5_aokvqa_better_sampled_q10_i10_s0\n",
      "Stage\t\tUnsure Rate\tUtility over previous stage\n",
      "Explanation\t   0.0%\t\t +0.000 ± 0.000\n",
      "Quality\t\t   5.3%\t\t +2.267 ± 0.091\n",
      "Error computing confusion matrix or M1–M4 scores: -3\n",
      "This would be expected if setting vf_descriptive_llava1.5_aokvqa_better_sampled_q10_i10_s0 is combined12 or combined123.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_16134/604509645.py:143: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_valid = df_valid.groupby('batch').apply(lambda g: g.head(30)).reset_index(drop=True)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import glob, os\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# ---------------------------\n",
    "# Helper Functions\n",
    "# ---------------------------\n",
    "def compute_se(series):\n",
    "    \"\"\"Compute the standard error on a pandas Series.\"\"\"\n",
    "    n = series.count()\n",
    "    return series.std(ddof=1) / np.sqrt(n) if n > 1 else 0\n",
    "\n",
    "def compute_metrics(df, stage):\n",
    "    \"\"\"\n",
    "    Given a DataFrame of interactions (with a column for the stage and a column for ground_truth),\n",
    "    compute:\n",
    "      - Total accuracy (unsure responses, coded as 2, count as incorrect)\n",
    "      - Accuracy on non-unsure responses (\"NotUnsureAcc\")\n",
    "      - Unsure rate\n",
    "      - Utility (0 if unsure; 1 if correct; -1 if incorrect)\n",
    "    Returns also the computed utility as a Series.\n",
    "    \"\"\"\n",
    "    preds = df[stage]\n",
    "    # Total accuracy: count unsure as incorrect.\n",
    "    correct_all = ((preds != 2) & (preds == df['ground_truth'])).astype(float)\n",
    "    total_acc_mean = correct_all.mean()\n",
    "    total_acc_se = compute_se(correct_all)\n",
    "    \n",
    "    # Accuracy on non-unsure responses.\n",
    "    non_unsure = preds != 2\n",
    "    if non_unsure.sum() > 0:\n",
    "        correct_non_unsure = (df.loc[non_unsure, stage] == df.loc[non_unsure, 'ground_truth']).astype(float)\n",
    "        acc_mean = correct_non_unsure.mean()\n",
    "        acc_se = compute_se(correct_non_unsure)\n",
    "    else:\n",
    "        acc_mean, acc_se = np.nan, np.nan\n",
    "\n",
    "    # Unsure rate.\n",
    "    unsure_indicator = (preds == 2).astype(float)\n",
    "    unsure_mean = unsure_indicator.mean()\n",
    "    unsure_se = compute_se(unsure_indicator)\n",
    "    \n",
    "    # Utility: 0 if unsure; else 1 if correct, -1 if incorrect.\n",
    "    # (i.e., 1 - 2*abs(pred - ground_truth))\n",
    "    def calc_util(row):\n",
    "        p = row[stage]\n",
    "        gt = row['ground_truth']\n",
    "        return 0 if p == 2 else 1 - 2*abs(p - gt)\n",
    "    utility = df.apply(calc_util, axis=1)\n",
    "    util_mean = utility.mean()\n",
    "    util_se = compute_se(utility)\n",
    "    \n",
    "    return total_acc_mean, total_acc_se, acc_mean, acc_se, unsure_mean, unsure_se, util_mean, util_se, utility\n",
    "\n",
    "# ---------------------------\n",
    "# Settings and Directories\n",
    "# ---------------------------\n",
    "settings = [\n",
    "    'combined12_llava1.5_aokvqa_better_sampled_q10_i10_s0',\n",
    "    'prodmetric_llava1.5_aokvqa_better_sampled_q10_i10_s0',\n",
    "    'vf_numeric_llava1.5_aokvqa_better_sampled_q10_i10_s0',\n",
    "    'vf_descriptive_llava1.5_aokvqa_better_sampled_q10_i10_s0',\n",
    "]\n",
    "USER_STUDIES_DIR = \"/home/shared/vlm_rationales_eval/user_studies_data/\"\n",
    "\n",
    "# To store scores and confusion matrices across settings\n",
    "all_setting_scores = []\n",
    "all_confusion_matrices = []\n",
    "consistent_choice_threshold = 0.9  # 90% threshold\n",
    "df_map = {}\n",
    "\n",
    "# ---------------------------\n",
    "# Process Each Setting via DataFrame\n",
    "# ---------------------------\n",
    "for setting in settings:\n",
    "    print(f\"\\nProcessing setting: {setting}\")\n",
    "    pattern = os.path.join(USER_STUDIES_DIR, \"prolific_batches\", \"batch_interaction_data\", setting, \"*.json\")\n",
    "    files = glob.glob(pattern)\n",
    "    \n",
    "    # Create a list of records (each record is an interaction)\n",
    "    records = []\n",
    "    for file in files:\n",
    "        batch_id = os.path.basename(file).split('.')[0]\n",
    "        with open(file) as f:\n",
    "            batch_data = json.load(f)\n",
    "        # Each file contains sessions for multiple users.\n",
    "        for user_id, session in batch_data.items():\n",
    "            interactions = session['interactions']\n",
    "            for interaction in interactions:\n",
    "                rec = {\n",
    "                    'user_id': user_id,\n",
    "                    'batch': batch_id,\n",
    "                    'question_i': interaction['question_i'],\n",
    "                    'answeronly': interaction['user_selections'].get('answeronly'),\n",
    "                    'withexplanation': interaction['user_selections'].get('withexplanation'),\n",
    "                    'withexplanationquality': interaction['user_selections'].get('withexplanationquality'),\n",
    "                    # Ground truth: defined as 1 - prediction_is_correct.\n",
    "                    'ground_truth': 1 - interaction['question']['prediction_is_correct']\n",
    "                }\n",
    "                records.append(rec)\n",
    "    \n",
    "    # Build DataFrame from the records.\n",
    "    df = pd.DataFrame(records)\n",
    "    print(f\"Found {df['user_id'].nunique()} users and {len(df)} interactions for setting '{setting}'.\")\n",
    "    \n",
    "    # ---------------------------\n",
    "    # Filter Users Based on Consistency\n",
    "    # ---------------------------\n",
    "    # For each user, look at all responses across all stages and drop users\n",
    "    # where one response (0, 1, or 2) comprises ≥90% of their responses.\n",
    "    def user_is_valid(group):\n",
    "        responses = pd.concat([group['answeronly'], group['withexplanation'], group['withexplanationquality']])\n",
    "        if responses.empty:\n",
    "            return False\n",
    "        norm_counts = responses.value_counts(normalize=True)\n",
    "        if norm_counts.max() >= consistent_choice_threshold:\n",
    "            # If desired, you could print which user is filtered out.\n",
    "            # print(f\"User {group.name} filtered out; response distribution: {norm_counts.to_dict()}\")\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    df_valid = df.groupby('user_id').filter(user_is_valid)\n",
    "\n",
    "    if df_valid.empty:\n",
    "        print(\"No interactions remain after filtering. Skipping evaluation for this setting.\")\n",
    "        continue\n",
    "        \n",
    "    # ---------------------------\n",
    "    # Filter Batches with More Than 30 Annotations\n",
    "    # ---------------------------\n",
    "    # For each batch, if it has been annotated more than 30 times (i.e. more than three users,\n",
    "    # assuming each user annotates about 10 interactions), print the annotation count\n",
    "    # and then shorten the batch to include only the first 30 annotations.\n",
    "    batch_counts = df_valid['batch'].value_counts()\n",
    "    for batch, count in batch_counts.items():\n",
    "        if count > 30:\n",
    "            print(f\"Batch {batch} has been annotated {count} times, exceeding 30 annotations. Shortening to first 30 annotations.\")\n",
    "    # Group by 'batch' and take only the first 30 entries for each batch\n",
    "    df_valid = df_valid.groupby('batch').apply(lambda g: g.head(30)).reset_index(drop=True)\n",
    "    print(f\"{df_valid['user_id'].nunique()} users remain after filtering; {len(df_valid)} interactions remain.\")\n",
    "    \n",
    "    \n",
    "    # Indicate which batches need to add users\n",
    "    batch_counts = df_valid['batch'].value_counts()\n",
    "    for batch, count in batch_counts.items():\n",
    "        if count % 10: raise ValueError('# Annotation should be divisible by 10, but found not??')\n",
    "        count = count // 10\n",
    "        if count > 3:\n",
    "            raise ValueError(f\"After filtering, # users taking the batch should not > 3??\")\n",
    "        diff = 3 - count\n",
    "        if diff != 0:\n",
    "            print(f\"Batch {batch} need {diff} more users to annotate.\")\n",
    "            \n",
    "    # Add df to the df_map for later use.\n",
    "    df_map[setting] = df_valid\n",
    "    \n",
    "    \n",
    "    # ---------------------------\n",
    "    # Compute Metrics for Each Stage Using DataFrames\n",
    "    # ---------------------------\n",
    "    stage_names = ['Answer Only', 'With Explanation', 'With Explanation + Quality']\n",
    "    stages = ['answeronly', 'withexplanation', 'withexplanationquality']\n",
    "    metrics = {}\n",
    "    utilities = {}\n",
    "    csv_prints = []\n",
    "    print(\"\\nStage                    \\tUnsure Rate\\tTotalAcc\\tNotUnsureAcc\\tUtility\")\n",
    "    for stage, name in zip(stages, stage_names):\n",
    "        tot_acc, tot_se, acc_mean, acc_se, unsure_mean, unsure_se, util_mean, util_se, util_series = compute_metrics(df_valid, stage)\n",
    "        metrics[stage] = {\n",
    "            'total_accuracy_mean': tot_acc,\n",
    "            'total_accuracy_se': tot_se,\n",
    "            'accuracy_mean': acc_mean,\n",
    "            'accuracy_se': acc_se,\n",
    "            'unsure_rate_mean': unsure_mean,\n",
    "            'unsure_rate_se': unsure_se,\n",
    "            'utility_mean': util_mean,\n",
    "            'utility_se': util_se\n",
    "        }\n",
    "        utilities[stage] = util_series\n",
    "        print(f\"{name:<25}\\t{unsure_mean*100:5.1f}% ± {unsure_se*100:4.1f}%\\t\"\n",
    "              f\"{tot_acc*100:5.1f}% ± {tot_se*100:4.1f}%\\t\"\n",
    "              f\"{acc_mean*100:5.1f}% ± {acc_se*100:4.1f}%\\t\"\n",
    "              f\"{util_mean:+.3f} ± {util_se:.3f}\")\n",
    "        csv_prints.append(f\"{unsure_mean*100:5.1f}% ± {unsure_se*100:4.1f}%, \"\n",
    "              f\"{tot_acc*100:5.1f}% ± {tot_se*100:4.1f}%, \"\n",
    "              f\"{acc_mean*100:5.1f}% ± {acc_se*100:4.1f}%, \"\n",
    "              f\"{util_mean:+.3f} ± {util_se:.3f}\")\n",
    "        \n",
    "    print(\", \".join(csv_prints))\n",
    "\n",
    "    # ---------------------------\n",
    "    # Utility Gains Between Stages\n",
    "    # ---------------------------\n",
    "    gain_explanation = utilities['withexplanation'] - utilities['answeronly']\n",
    "    gain_quality = utilities['withexplanationquality'] - utilities['withexplanation']\n",
    "    expl_gain_mean = gain_explanation.mean()\n",
    "    expl_gain_se = gain_explanation.std(ddof=1) / np.sqrt(len(gain_explanation)) if len(gain_explanation) > 1 else 0\n",
    "    qual_gain_mean = gain_quality.mean()\n",
    "    qual_gain_se = gain_quality.std(ddof=1) / np.sqrt(len(gain_quality)) if len(gain_quality) > 1 else 0\n",
    "    \n",
    "    print(f\"\\nSetting: {setting}\")\n",
    "    print(\"Stage\\t\\tUnsure Rate\\tUtility over previous stage\")\n",
    "    print(f\"Explanation\\t {metrics['withexplanation']['unsure_rate_mean']*100:5.1f}%\\t\\t {expl_gain_mean:+.3f} ± {expl_gain_se:.3f}\")\n",
    "    print(f\"Quality\\t\\t {metrics['withexplanationquality']['unsure_rate_mean']*100:5.1f}%\\t\\t {qual_gain_mean:+.3f} ± {qual_gain_se:.3f}\")\n",
    "    \n",
    "    # ---------------------------\n",
    "    # Compute Confusion Matrix and M1–M4 Scores\n",
    "    # ---------------------------\n",
    "    # Compute utility for withexplanation and withexplanationquality directly.\n",
    "    def compute_utility(val, gt):\n",
    "        return 0 if val == 2 else 1 - 2*abs(val-gt)\n",
    "    \n",
    "    df_valid['util_withexplanation'] = df_valid.apply(lambda row: compute_utility(row['withexplanation'], row['ground_truth']), axis=1)\n",
    "    df_valid['util_withexplanationquality'] = df_valid.apply(lambda row: compute_utility(row['withexplanationquality'], row['ground_truth']), axis=1)\n",
    "    \n",
    "    # For the confusion matrix, map utilities to indices: -1 -> 0, 0 -> 1, 1 -> 2.\n",
    "    mapping = {-1: 0, 0: 1, 1: 2}\n",
    "    label_order = [-1, 0, 1]\n",
    "    label_names = ['Incorrect', 'Unsure', 'Correct']\n",
    "    \n",
    "    try:\n",
    "        cm = np.zeros((3, 3), dtype=int)\n",
    "        for _, row in df_valid.iterrows():\n",
    "            u2 = row['util_withexplanation']\n",
    "            u3 = row['util_withexplanationquality']\n",
    "            cm[mapping[u2], mapping[u3]] += 1\n",
    "        cm_transposed = cm.T\n",
    "    except Exception as e:\n",
    "        print(f\"Error computing confusion matrix or M1–M4 scores: {e}\")\n",
    "        print(f\"This would be expected if setting {setting} is combined12 or combined123.\")\n",
    "        continue\n",
    "        \n",
    "    print(f\"\\nSetting: {setting} Step 2 -> Step 3 (transposed)\")\n",
    "    print(\"\\t\\t\" + \"\\t\".join(label_names))\n",
    "    for i, label in enumerate(label_names):\n",
    "        row_values = \"\\t\".join(str(cm_transposed[i, j]) for j in range(3))\n",
    "        print(f\"{label:<10}\\t {row_values}\")\n",
    "    \n",
    "    # Compute M1–M4 using counts from the confusion matrix.\n",
    "    def count_transitions(val2, val3):\n",
    "        return ((df_valid['util_withexplanation'] == val2) & (df_valid['util_withexplanationquality'] == val3)).sum()\n",
    "    \n",
    "    denom_M1 = count_transitions(0, -1) + count_transitions(0, 0) + count_transitions(0, 1)\n",
    "    num_M1 = count_transitions(0, -1) + count_transitions(0, 1)\n",
    "    m1 = num_M1 / denom_M1 if denom_M1 > 0 else np.nan\n",
    "\n",
    "    denom_M2 = count_transitions(0, -1) + count_transitions(0, 1)\n",
    "    m2 = count_transitions(0, 1) / denom_M2 if denom_M2 > 0 else np.nan\n",
    "\n",
    "    denom_M3 = count_transitions(-1, -1) + count_transitions(-1, 0) + count_transitions(-1, 1)\n",
    "    num_M3 = count_transitions(-1, 0) + count_transitions(-1, 1)\n",
    "    m3 = num_M3 / denom_M3 if denom_M3 > 0 else np.nan\n",
    "\n",
    "    denom_M4 = count_transitions(1, -1) + count_transitions(1, 0) + count_transitions(1, 1)\n",
    "    num_M4 = count_transitions(1, -1) + count_transitions(1, 0)\n",
    "    m4 = num_M4 / denom_M4 if denom_M4 > 0 else np.nan\n",
    "\n",
    "    print(\"\\nM1–M4 Scores:\")\n",
    "    print(f\"M1 (P(S3 ≠ U | S2 = U)): {m1:.2%}\")\n",
    "    print(f\"M2 (P(S3 = C | S2 = U, S3 ≠ U)): {m2:.2%}\")\n",
    "    print(f\"M3 (P(S3 ≠ I | S2 = I)): {m3:.2%}\")\n",
    "    print(f\"M4 (P(S3 ≠ C | S2 = C)): {m4:.2%}\")\n",
    "    print(f\"{m1:.2%}, {m2:.2%}, {m3:.2%}, {m4:.2%}, {m3-m4:.2%}\")\n",
    "\n",
    "    \n",
    "    # ---------------------------\n",
    "    # Print Examples Where Step 2 is Correct and Step 3 is Incorrect\n",
    "    # ---------------------------\n",
    "    print(\"\\nExamples where Step 2 is correct and Step 3 is incorrect:\")\n",
    "    examples = df_valid[\n",
    "        (df_valid['withexplanation'] != 2) &\n",
    "        (df_valid['withexplanation'] == df_valid['ground_truth']) &\n",
    "        (df_valid['withexplanationquality'] != 2) &\n",
    "        (df_valid['withexplanationquality'] != df_valid['ground_truth'])\n",
    "    ]\n",
    "    if not examples.empty:\n",
    "        for idx, row in examples.head(5).iterrows():\n",
    "            print(row.to_json())\n",
    "    else:\n",
    "        print(\"No examples found.\")\n",
    "    \n",
    "    # ---------------------------\n",
    "    # Plot: Confusion Matrix and M1–M4 Scores for the Current Setting\n",
    "    # ---------------------------\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12, 5))\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    ax = axes[0]\n",
    "    im = ax.imshow(cm_transposed, cmap='Greens', interpolation='nearest')\n",
    "    ax.set_title(f\"Setting {setting.split('_llava')[0]} Step 2->Step 3\\n(WithExplanation -> WithExplanationQuality)\")\n",
    "    ax.set_xticks(np.arange(len(label_names)))\n",
    "    ax.set_yticks(np.arange(len(label_names)))\n",
    "    ax.set_xticklabels(label_names)\n",
    "    ax.set_yticklabels(label_names)\n",
    "    ax.set_xlabel(\"WithExplanation\")\n",
    "    ax.set_ylabel(\"WithExplanationQuality\")\n",
    "    for i in range(len(label_names)):\n",
    "        for j in range(len(label_names)):\n",
    "            ax.text(j, i, cm_transposed[i, j], ha=\"center\", va=\"center\", color=\"black\")\n",
    "    \n",
    "    # Plot M1–M4 scores as text.\n",
    "    ax = axes[1]\n",
    "    ax.axis('off')\n",
    "    score_text = (f\"M1 (P(S3 ≠ U | S2 = U)): {m1:.2%}\\n\"\n",
    "                  f\"M2 (P(S3 = C | S2 = U, S3 ≠ U)): {m2:.2%}\\n\"\n",
    "                  f\"M3 (P(S3 ≠ I | S2 = I)): {m3:.2%}\\n\"\n",
    "                  f\"M4 (P(S3 ≠ C | S2 = C)): {m4:.2%}\")\n",
    "    ax.text(0.5, 0.5, score_text, fontsize=12, ha='center', va='center', transform=ax.transAxes)\n",
    "    ax.set_title(\"M1–M4 Scores\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Save scores and confusion matrix for later comparison/plotting.\n",
    "    all_setting_scores.append({\n",
    "        \"Setting\": setting,\n",
    "        \"Resolution Rate\": m1,\n",
    "        \"Positive Conversion Rate\": m2,\n",
    "        \"Error Recovery Rate\": m3,\n",
    "        \"Correct Loss Rate\": m4\n",
    "    })\n",
    "    all_confusion_matrices.append((setting, cm_transposed))\n",
    "\n",
    "# ---------------------------\n",
    "# Plot Grouped Bar Chart for M1–M4 Scores Across Settings\n",
    "# ---------------------------\n",
    "if all_setting_scores:\n",
    "    df_scores = pd.DataFrame(all_setting_scores)\n",
    "    df_scores[\"SettingShort\"] = df_scores[\"Setting\"].apply(lambda s: s.split('_llava')[0])\n",
    "    df_scores[\"Recovery Gap\"] = df_scores[\"Error Recovery Rate\"] - df_scores[\"Correct Loss Rate\"]\n",
    "    df_scores = df_scores.set_index(\"SettingShort\")\n",
    "    \n",
    "    # Transpose so that rows become score types.\n",
    "    df_scores_T = df_scores[[\"Resolution Rate\", \"Positive Conversion Rate\", \"Error Recovery Rate\",\n",
    "                             \"Correct Loss Rate\", \"Recovery Gap\"]].transpose()\n",
    "    \n",
    "    ax = df_scores_T.plot(kind='bar', figsize=(12, 6))\n",
    "    ax.set_ylabel(\"Score (Fraction)\")\n",
    "    ax.set_title(\"Scores by Score Type Across Settings\")\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), rotation=45)\n",
    "    ax.legend(title=\"Setting\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# ---------------------------\n",
    "# Plot a Grid of Confusion Matrices for Every Setting\n",
    "# ---------------------------\n",
    "if all_confusion_matrices:\n",
    "    n_settings = len(all_confusion_matrices)\n",
    "    ncols = 2\n",
    "    nrows = math.ceil(n_settings / ncols)\n",
    "    fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(ncols*5, nrows*5))\n",
    "    if n_settings > 1:\n",
    "        axes = np.array(axes).flatten()\n",
    "    else:\n",
    "        axes = [axes]\n",
    "        \n",
    "    for ax, (setting, cm_transposed) in zip(axes, all_confusion_matrices):\n",
    "        im = ax.imshow(cm_transposed, cmap='Greens', interpolation='nearest')\n",
    "        ax.set_title(setting.split('_llava')[0])\n",
    "        ax.set_xticks(np.arange(len(label_names)))\n",
    "        ax.set_yticks(np.arange(len(label_names)))\n",
    "        ax.set_xticklabels(label_names)\n",
    "        ax.set_yticklabels(label_names)\n",
    "        ax.set_xlabel(\"WithExplanation\")\n",
    "        ax.set_ylabel(\"WithExplanationQuality\")\n",
    "        for i in range(len(label_names)):\n",
    "            for j in range(len(label_names)):\n",
    "                ax.text(j, i, cm_transposed[i, j], ha=\"center\", va=\"center\", color=\"black\")\n",
    "                \n",
    "    # Turn off any extra axes.\n",
    "    for ax in axes[len(all_confusion_matrices):]:\n",
    "        ax.axis('off')\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store df_map to a file for later use.\n",
    "import pickle\n",
    "for setting, df in df_map.items():\n",
    "    # only keep the columns we need\n",
    "    df_map[setting] = df[['user_id', 'batch', 'question_i', 'withexplanationquality', 'ground_truth']]\n",
    "with open('df_combined123_map.pkl', 'wb') as f:\n",
    "    pickle.dump(df_map, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start from here for the analysis of user interaction data vs. confidence score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# Load df_map from the file.\n",
    "with open('df_map.pkl', 'rb') as f:\n",
    "    df_map = pickle.load(f)\n",
    "print(\"Loaded df_map from file.\")\n",
    "# print the keys of df_map\n",
    "print(\"Keys in df_map:\")\n",
    "for key in df_map.keys():\n",
    "    print(key)\n",
    "df_map['vf_numeric_llava1.5_with_image_q20_i10_s0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded df_combined123_map from file.\n",
      "Keys in df_combined123_map:\n",
      "combined12_llava1.5_aokvqa_better_sampled_q10_i10_s0\n",
      "prodmetric_llava1.5_aokvqa_better_sampled_q10_i10_s0\n",
      "vf_numeric_llava1.5_aokvqa_better_sampled_q10_i10_s0\n",
      "vf_descriptive_llava1.5_aokvqa_better_sampled_q10_i10_s0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>batch</th>\n",
       "      <th>question_i</th>\n",
       "      <th>withexplanationquality</th>\n",
       "      <th>ground_truth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>63483c4522d65d46028da7a7</td>\n",
       "      <td>000_3users</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>63483c4522d65d46028da7a7</td>\n",
       "      <td>000_3users</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>63483c4522d65d46028da7a7</td>\n",
       "      <td>000_3users</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>63483c4522d65d46028da7a7</td>\n",
       "      <td>000_3users</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>63483c4522d65d46028da7a7</td>\n",
       "      <td>000_3users</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>672a6ed8d37a4143e096929b</td>\n",
       "      <td>009_3users</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>672a6ed8d37a4143e096929b</td>\n",
       "      <td>009_3users</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>672a6ed8d37a4143e096929b</td>\n",
       "      <td>009_3users</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>672a6ed8d37a4143e096929b</td>\n",
       "      <td>009_3users</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>672a6ed8d37a4143e096929b</td>\n",
       "      <td>009_3users</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>300 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      user_id       batch  question_i  withexplanationquality  \\\n",
       "0    63483c4522d65d46028da7a7  000_3users           0                       0   \n",
       "1    63483c4522d65d46028da7a7  000_3users           1                       0   \n",
       "2    63483c4522d65d46028da7a7  000_3users           2                       0   \n",
       "3    63483c4522d65d46028da7a7  000_3users           3                       0   \n",
       "4    63483c4522d65d46028da7a7  000_3users           4                       0   \n",
       "..                        ...         ...         ...                     ...   \n",
       "295  672a6ed8d37a4143e096929b  009_3users           5                       0   \n",
       "296  672a6ed8d37a4143e096929b  009_3users           6                       0   \n",
       "297  672a6ed8d37a4143e096929b  009_3users           7                       0   \n",
       "298  672a6ed8d37a4143e096929b  009_3users           8                       0   \n",
       "299  672a6ed8d37a4143e096929b  009_3users           9                       0   \n",
       "\n",
       "     ground_truth  \n",
       "0               0  \n",
       "1               0  \n",
       "2               0  \n",
       "3               0  \n",
       "4               1  \n",
       "..            ...  \n",
       "295             1  \n",
       "296             0  \n",
       "297             0  \n",
       "298             0  \n",
       "299             1  \n",
       "\n",
       "[300 rows x 5 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "# Load df_combined123_map from the file.\n",
    "with open('df_combined123_map.pkl', 'rb') as f:\n",
    "    df_combined123_map = pickle.load(f)\n",
    "print(\"Loaded df_combined123_map from file.\")\n",
    "# print the keys of df_combined123_map\n",
    "print(\"Keys in df_combined123_map:\")\n",
    "for key in df_combined123_map.keys():\n",
    "    print(key)\n",
    "df_combined123_map['vf_numeric_llava1.5_aokvqa_better_sampled_q10_i10_s0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 1) Some helper functions\n",
    "def calc_util(row):\n",
    "    p  = row['withexplanationquality']    # 0=trust, 1=distrust, 2=unsure\n",
    "    gt = row['ground_truth']              # 0=correct, 1=incorrect\n",
    "    if p == 2:\n",
    "        return 0\n",
    "    # for p in {0,1}, utility = +1 if (p==gt) else –1\n",
    "    return 1 - 2 * abs(p - gt)\n",
    "\n",
    "# 2) For each model‐variant DF, compute metrics\n",
    "rows = []\n",
    "for key, df in df_combined123_map.items():\n",
    "    n = len(df)\n",
    "    p  = df['withexplanationquality']\n",
    "    gt = df['ground_truth']\n",
    "    \n",
    "    unsure_count  = (p == 2).sum()\n",
    "    accept_count  = (p == 0).sum()\n",
    "    \n",
    "    # “Correct predictions” = user trusts correct OR user rejects incorrect\n",
    "    correct_preds = ((p == 0) & (gt == 0) | (p == 1) & (gt == 1)).sum()\n",
    "    \n",
    "    # Rates\n",
    "    unsure_rate     = unsure_count / n\n",
    "    accept_rate     = accept_count / n\n",
    "    total_acc       = correct_preds / n\n",
    "    not_unsure_acc  = correct_preds / (n - unsure_count) if (n - unsure_count)>0 else np.nan\n",
    "    false_acc_rate  = ((p == 0) & (gt == 1)).sum() / (gt == 1).sum()\n",
    "    false_rej_rate  = ((p == 1) & (gt == 0)).sum() / (gt == 0).sum()\n",
    "    \n",
    "    # Utility per row -> mean\n",
    "    df['utility']   = df.apply(calc_util, axis=1)\n",
    "    mean_util       = df['utility'].mean()\n",
    "    \n",
    "    rows.append({\n",
    "        'model_variant': key,\n",
    "        'Unsure Rate':    unsure_rate,\n",
    "        'Accept Rate':    accept_rate,\n",
    "        'TotalAcc':       total_acc,\n",
    "        'NotUnsureAcc':   not_unsure_acc,\n",
    "        'False Acc Rate': false_acc_rate,\n",
    "        'False Rej Rate': false_rej_rate,\n",
    "        'Mean Utility':   mean_util\n",
    "    })\n",
    "\n",
    "# 3) Summarize\n",
    "summary_df = pd.DataFrame(rows).set_index('model_variant')\n",
    "display(summary_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- Settings to compare ---\n",
    "settings = [\n",
    "    'vf_numeric_llava1.5_with_image_q20_i10_s0',\n",
    "    'contr_numeric_llava1.5_with_image_q20_i10_s0',\n",
    "    'avg_vf_contr_numeric_llava1.5_with_image_q20_i10_s0',\n",
    "    'vf_descriptive_llava1.5_with_image_q20_i10_s0',\n",
    "    'contr_descriptive_llava1.5_with_image_q20_i10_s0',\n",
    "    'vf_numeric_qwen2.5_vizwiz_q10_i10_s0',\n",
    "    'vf_descriptive_qwen2.5_vizwiz_q10_i10_s0',\n",
    "]\n",
    "\n",
    "records = []\n",
    "for setting in settings:\n",
    "    df = df_map[setting]\n",
    "    for idx, row in df.iterrows():\n",
    "        queue_name = \"qwen2.5_vizwiz_q10_i10_s0\" if 'vizwiz' in setting else \"llava1.5_with_image_q20_i10_s0\"\n",
    "        # get the batch ID (e.g. '000' from '000_3users')\n",
    "        batch_id = row['batch'].split('_')[0]\n",
    "        path = f'../web/baked_queues/{queue_name}/{batch_id}.json'\n",
    "        if not os.path.exists(path):\n",
    "            continue\n",
    "\n",
    "        # load the questions for this batch\n",
    "        with open(path) as f:\n",
    "            questions = json.load(f)\n",
    "        entry = questions[row['question_i']]\n",
    "\n",
    "        vf   = entry.get('visual_fidelity')\n",
    "        ctr  = entry.get('contrastiveness')\n",
    "        avg  = None if vf is None or ctr is None else (vf + ctr) / 2\n",
    "\n",
    "        # pick which score was shown\n",
    "        if 'avg_vf_contr' in setting:\n",
    "            confidence = avg\n",
    "        elif setting.startswith('vf'):\n",
    "            confidence = vf\n",
    "        elif setting.startswith('contr'):\n",
    "            confidence = ctr\n",
    "        else:\n",
    "            condidence = None\n",
    "        # add to df\n",
    "        df.loc[idx, 'confidence'] = confidence\n",
    "    \n",
    "df_map['vf_numeric_llava1.5_with_image_q20_i10_s0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# mapping from your raw column names to nice labels\n",
    "stage_map = {\n",
    "    'answeronly': 'Answer Only',\n",
    "    'withexplanation': 'With Explanation',\n",
    "    'withexplanationquality': 'With Explanation + Quality'\n",
    "}\n",
    "\n",
    "selection_order = [0, 1, 2]\n",
    "colors = ['tab:green', 'tab:red', 'tab:orange']\n",
    "stage_order = list(stage_map.values())\n",
    "\n",
    "for setting, df in df_map.items():\n",
    "    # 1) melt into long form\n",
    "    df_long = df.melt(\n",
    "        id_vars=['user_id', 'batch', 'question_i', 'ground_truth'],\n",
    "        value_vars=list(stage_map.keys()),\n",
    "        var_name='Stage',\n",
    "        value_name='Selection'\n",
    "    )\n",
    "    # 2) map raw to pretty\n",
    "    df_long['Stage'] = df_long['Stage'].map(stage_map)\n",
    "\n",
    "    # 3) group + count\n",
    "    grouped = (\n",
    "        df_long\n",
    "        .groupby('Stage')['Selection']\n",
    "        .value_counts()\n",
    "        .unstack(fill_value=0)\n",
    "        .reindex(index=stage_order, columns=selection_order)\n",
    "    )\n",
    "\n",
    "    # 4) plot stacked bar\n",
    "    fig, ax = plt.subplots(figsize=(6, 8))\n",
    "    x = np.arange(len(stage_order))\n",
    "    bottom = np.zeros(len(stage_order))\n",
    "\n",
    "    for sel, color in zip(selection_order, colors):\n",
    "        counts = grouped[sel].values\n",
    "        legend_label = f\"Correct\" if sel == 0 else f\"Incorrect\" if sel == 1 else \"Unsure\"\n",
    "        bars = ax.bar(x, counts, bottom=bottom,\n",
    "                      label=legend_label,\n",
    "                      color=color, edgecolor='black')\n",
    "        # annotate counts\n",
    "        for idx, bar in enumerate(bars):\n",
    "            h = bar.get_height()\n",
    "            if h:\n",
    "                ax.text(\n",
    "                    bar.get_x() + bar.get_width()/2,\n",
    "                    bottom[idx] + h/2,\n",
    "                    f\"{int(h)}\",\n",
    "                    ha='center', va='center'\n",
    "                )\n",
    "        bottom += counts\n",
    "\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(stage_order, fontsize=12, rotation=30)\n",
    "    ax.set_xlabel(\"Stage\", fontsize=14)\n",
    "    ax.set_ylabel(\"Count\", fontsize=14)\n",
    "    ax.set_title(f\"Selection Counts by Stage ({setting.split('_llava')[0].split('_vizwiz')[0]})\",\n",
    "                 fontsize=16)\n",
    "    ax.legend(fontsize=12)\n",
    "    ax.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# mapping from your raw column names to nice labels\n",
    "stage_map = {\n",
    "    'answeronly': 'Answer Only',\n",
    "    'withexplanation': 'With Explanation',\n",
    "    'withexplanationquality': 'With Explanation + Quality'\n",
    "}\n",
    "\n",
    "selection_order = [0, 1, 2]\n",
    "colors = ['tab:green', 'tab:red', 'tab:orange']\n",
    "stage_order = list(stage_map.values())\n",
    "\n",
    "for setting, df in df_map.items():\n",
    "    # 1) melt into long form\n",
    "    df_long = df.melt(\n",
    "        id_vars=['user_id', 'batch', 'question_i', 'ground_truth'],\n",
    "        value_vars=list(stage_map.keys()),\n",
    "        var_name='Stage',\n",
    "        value_name='Selection'\n",
    "    )\n",
    "    # 2) map raw to pretty\n",
    "    df_long['Stage'] = df_long['Stage'].map(stage_map)\n",
    "\n",
    "    # 3) compute u_i\n",
    "    #   u_i = 0 if Selection == 2\n",
    "    #   u_i = 1 if (sel=1 & gt=0) or (sel=0 & gt=1)\n",
    "    #   u_i = -1 otherwise (i.e. sel matches gt)\n",
    "    df_long['u_i'] = np.where(\n",
    "        df_long['Selection'] == 2, 0,\n",
    "        np.where(\n",
    "            ((df_long['Selection'] == 1) & (df_long['ground_truth'] == 1)) |\n",
    "            ((df_long['Selection'] == 0) & (df_long['ground_truth'] == 0)),\n",
    "            1,\n",
    "            -1\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # inspect the new column\n",
    "    print(f\"\\n—— {setting} ——\")\n",
    "    # randomly sample 5 rows\n",
    "    print(df_long[['Stage', 'Selection', 'ground_truth', 'u_i']].sample(5))\n",
    "\n",
    "    # now group & plot u_i\n",
    "    u_order  = [1, -1, 0]                     # mismatch, unsure, match\n",
    "    u_colors = ['tab:green', 'tab:red', 'tab:orange',]\n",
    "    u_labels = ['Match (+1)', 'Mismatch (-1)', 'Unsure (0)',]\n",
    "\n",
    "    grouped = (\n",
    "        df_long\n",
    "        .groupby('Stage')['u_i']\n",
    "        .value_counts()\n",
    "        .unstack(fill_value=0)\n",
    "        .reindex(index=stage_order, columns=u_order)\n",
    "    )\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(6, 8))\n",
    "    x = np.arange(len(stage_order))\n",
    "    bottom = np.zeros(len(stage_order))\n",
    "\n",
    "    for u, color, label in zip(u_order, u_colors, u_labels):\n",
    "        counts = grouped[u].values\n",
    "        bars = ax.bar(x, counts, bottom=bottom, label=label,\n",
    "                    color=color, edgecolor='black')\n",
    "        for idx, bar in enumerate(bars):\n",
    "            h = bar.get_height()\n",
    "            if h:\n",
    "                ax.text(bar.get_x() + bar.get_width()/2,\n",
    "                        bottom[idx] + h/2,\n",
    "                        f\"{int(h)}\",\n",
    "                        ha='center', va='center')\n",
    "        bottom += counts\n",
    "\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(stage_order, fontsize=12, rotation=30)\n",
    "    ax.set_xlabel(\"Stage\", fontsize=14)\n",
    "    ax.set_ylabel(\"Count\", fontsize=14)\n",
    "    ax.set_title(f\"Answer Correctness by Stage ({setting.split('_llava')[0].split('_vizwiz')[0]})\",\n",
    "                fontsize=16)\n",
    "    ax.legend(fontsize=12, title=\"Correctness\", loc='upper left')\n",
    "    ax.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from matplotlib.ticker import PercentFormatter\n",
    "\n",
    "# --- your existing mappings & lists ---\n",
    "stage_map = {\n",
    "    'answeronly': 'Answer Only',\n",
    "    'withexplanation': 'With Explanation',\n",
    "    'withexplanationquality': 'With Explanation + Quality'\n",
    "}\n",
    "selection_order = [0, 1, 2]\n",
    "colors = ['tab:green', 'tab:red', 'tab:orange']\n",
    "labels = {\n",
    "    0: 'Correct',\n",
    "    1: 'Incorrect',\n",
    "    2: 'Unsure'\n",
    "}\n",
    "applied_settings = [\n",
    "    'vf_numeric_llava1.5_with_image_q20_i10_s0',\n",
    "    'contr_numeric_llava1.5_with_image_q20_i10_s0',\n",
    "    'avg_vf_contr_numeric_llava1.5_with_image_q20_i10_s0',\n",
    "    'showbothmetrics_llava1.5_with_image_q20_i10_s0',\n",
    "]\n",
    "applied_setting_names = {\n",
    "    'vf_numeric_llava1.5_with_image_q20_i10_s0': 'VF Numeric',\n",
    "    'contr_numeric_llava1.5_with_image_q20_i10_s0': 'CONTR Numeric',\n",
    "    'avg_vf_contr_numeric_llava1.5_with_image_q20_i10_s0': 'Average VF/CONTR',\n",
    "    'showbothmetrics_llava1.5_with_image_q20_i10_s0': 'Both Metrics',\n",
    "}\n",
    "\n",
    "# 1) Melt & compute u_i for all settings (same as before) …\n",
    "all_dfs = []\n",
    "for setting, df in df_map.items():\n",
    "    if setting not in applied_settings:\n",
    "        continue\n",
    "    df_long = df.melt(\n",
    "        id_vars=['user_id','batch','question_i','ground_truth'],\n",
    "        value_vars=list(stage_map.keys()),\n",
    "        var_name='Stage', value_name='Selection'\n",
    "    )\n",
    "    df_long['Stage'] = df_long['Stage'].map(stage_map)\n",
    "    df_long['Setting'] = setting\n",
    "\n",
    "    all_dfs.append(df_long)\n",
    "big = pd.concat(all_dfs, ignore_index=True)\n",
    "\n",
    "# 2a) Overall Stage 2 (“With Explanation”)\n",
    "agg2 = (big.loc[big['Stage']=='With Explanation','Selection']\n",
    "          .value_counts()\n",
    "          .reindex(selection_order, fill_value=0))\n",
    "agg2_prop = agg2 / agg2.sum()\n",
    "\n",
    "# 2b) Stage 3 (“With Explanation + Quality”) per setting\n",
    "stage3 = big[big['Stage']=='With Explanation + Quality']\n",
    "by_set = (stage3.groupby('Setting')['Selection']\n",
    "               .value_counts()\n",
    "               .unstack(fill_value=0)\n",
    "               .reindex(columns=selection_order, fill_value=0))\n",
    "by_set_prop = by_set.div(by_set.sum(axis=1), axis=0)\n",
    "\n",
    "# 3) Build combined_prop in the exact order: Overall, then each applied_setting\n",
    "rows = [agg2_prop] + [by_set_prop.loc[s] for s in applied_settings]\n",
    "index = ['Overall'] + applied_settings\n",
    "combined_prop = pd.DataFrame(rows, index=index, columns=selection_order)\n",
    "\n",
    "# 4) Plot them with tighter bars\n",
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "x = np.arange(len(combined_prop))\n",
    "width = 0.8   \n",
    "bottom = np.zeros(len(combined_prop))\n",
    "\n",
    "for u, color in zip(selection_order, colors):\n",
    "    vals = combined_prop[u]\n",
    "    bars = ax.bar(x, vals, width, bottom=bottom,\n",
    "                  color=color, label=labels[u], edgecolor='k')\n",
    "    for xi, (bar, v) in enumerate(zip(bars, vals)):\n",
    "        if v > 0:\n",
    "            ax.text(\n",
    "                bar.get_x() + bar.get_width()/2,\n",
    "                bottom[xi] + v/2,\n",
    "                f\"{v:.1%}\",\n",
    "                ha='center', va='center'\n",
    "            )\n",
    "    bottom += vals\n",
    "\n",
    "# 5) Force x‑labels in the same exact order\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(\n",
    "    ['With Explanation'] +\n",
    "    [applied_setting_names[s] for s in applied_settings],\n",
    "    rotation=30, ha='right'\n",
    ")\n",
    "\n",
    "ax.margins(x=0.02)\n",
    "ax.yaxis.set_major_formatter(PercentFormatter(xmax=1))\n",
    "ax.set_ylabel('Percentage', fontsize=14)\n",
    "ax.set_title('User Selection Across Settings', fontsize=16)\n",
    "ax.legend(title='Selection', loc='lower right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from matplotlib.ticker import PercentFormatter\n",
    "\n",
    "# --- your existing mappings & lists ---\n",
    "stage_map = {\n",
    "    'answeronly': 'Answer Only',\n",
    "    'withexplanation': 'With Explanation',\n",
    "    'withexplanationquality': 'With Explanation + Quality'\n",
    "}\n",
    "selection_order = [1, -1, 0]\n",
    "colors = ['tab:green', 'tab:red', 'tab:orange']\n",
    "labels = {\n",
    "    1: 'Match (+1)',\n",
    "   -1: 'Mismatch (-1)',\n",
    "    0: 'Unsure (0)'\n",
    "}\n",
    "applied_settings = [\n",
    "    'vf_numeric_llava1.5_with_image_q20_i10_s0',\n",
    "    'contr_numeric_llava1.5_with_image_q20_i10_s0',\n",
    "    'avg_vf_contr_numeric_llava1.5_with_image_q20_i10_s0',\n",
    "    'showbothmetrics_llava1.5_with_image_q20_i10_s0',\n",
    "]\n",
    "applied_setting_names = {\n",
    "    'vf_numeric_llava1.5_with_image_q20_i10_s0': 'VF Numeric',\n",
    "    'contr_numeric_llava1.5_with_image_q20_i10_s0': 'CONTR Numeric',\n",
    "    'avg_vf_contr_numeric_llava1.5_with_image_q20_i10_s0': 'Average VF/CONTR',\n",
    "    'showbothmetrics_llava1.5_with_image_q20_i10_s0': 'Both Metrics',\n",
    "}\n",
    "\n",
    "# 1) Melt & compute u_i for all settings (same as before) …\n",
    "all_dfs = []\n",
    "for setting, df in df_map.items():\n",
    "    if setting not in applied_settings:\n",
    "        continue\n",
    "    df_long = df.melt(\n",
    "        id_vars=['user_id','batch','question_i','ground_truth'],\n",
    "        value_vars=list(stage_map.keys()),\n",
    "        var_name='Stage', value_name='Selection'\n",
    "    )\n",
    "    df_long['Stage'] = df_long['Stage'].map(stage_map)\n",
    "    df_long['Setting'] = setting\n",
    "    df_long['u_i'] = np.where(\n",
    "        df_long['Selection']==2, 0,\n",
    "        np.where(\n",
    "            ((df_long['Selection']==1)&(df_long['ground_truth']==1)) |\n",
    "            ((df_long['Selection']==0)&(df_long['ground_truth']==0)),\n",
    "            1,\n",
    "            -1\n",
    "        )\n",
    "    )\n",
    "    all_dfs.append(df_long)\n",
    "big = pd.concat(all_dfs, ignore_index=True)\n",
    "\n",
    "# 2a) Overall Stage 2 (“With Explanation”)\n",
    "agg2 = (big.loc[big['Stage']=='With Explanation','u_i']\n",
    "          .value_counts()\n",
    "          .reindex(selection_order, fill_value=0))\n",
    "agg2_prop = agg2 / agg2.sum()\n",
    "\n",
    "# 2b) Stage 3 (“With Explanation + Quality”) per setting\n",
    "stage3 = big[big['Stage']=='With Explanation + Quality']\n",
    "by_set = (stage3.groupby('Setting')['u_i']\n",
    "               .value_counts()\n",
    "               .unstack(fill_value=0)\n",
    "               .reindex(columns=selection_order, fill_value=0))\n",
    "by_set_prop = by_set.div(by_set.sum(axis=1), axis=0)\n",
    "\n",
    "# 3) Build combined_prop in the exact order: Overall, then each applied_setting\n",
    "rows = [agg2_prop] + [by_set_prop.loc[s] for s in applied_settings]\n",
    "index = ['Overall'] + applied_settings\n",
    "combined_prop = pd.DataFrame(rows, index=index, columns=selection_order)\n",
    "\n",
    "# 4) Plot them with tighter bars\n",
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "x = np.arange(len(combined_prop))\n",
    "width = 0.8   \n",
    "bottom = np.zeros(len(combined_prop))\n",
    "\n",
    "for u, color in zip(selection_order, colors):\n",
    "    vals = combined_prop[u]\n",
    "    bars = ax.bar(x, vals, width, bottom=bottom,\n",
    "                  color=color, label=labels[u], edgecolor='k')\n",
    "    for xi, (bar, v) in enumerate(zip(bars, vals)):\n",
    "        if v > 0:\n",
    "            ax.text(\n",
    "                bar.get_x() + bar.get_width()/2,\n",
    "                bottom[xi] + v/2,\n",
    "                f\"{v:.1%}\",\n",
    "                ha='center', va='center'\n",
    "            )\n",
    "    bottom += vals\n",
    "\n",
    "# 5) Force x‑labels in the same exact order\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(\n",
    "    ['With Explanation'] +\n",
    "    [applied_setting_names[s] for s in applied_settings],\n",
    "    rotation=30, ha='right'\n",
    ")\n",
    "\n",
    "ax.margins(x=0.02)\n",
    "ax.yaxis.set_major_formatter(PercentFormatter(xmax=1))\n",
    "ax.set_ylabel('Percentage', fontsize=14)\n",
    "ax.set_title('Answer Correctness by Setting (in %)', fontsize=16)\n",
    "ax.legend(title='Correctness', loc='lower right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for setting, df in df_map.items():\n",
    "    df['utility_2'] = np.where(\n",
    "        df['withexplanation'] == 2, 0,\n",
    "        np.where(\n",
    "            ((df['withexplanation'] == 1) & (df['ground_truth'] == 1)) |\n",
    "            ((df['withexplanation'] == 0) & (df['ground_truth'] == 0)),\n",
    "            1,\n",
    "            -1\n",
    "        )\n",
    "    )\n",
    "    df['utility_3'] = np.where(\n",
    "        df['withexplanationquality'] == 2, 0,\n",
    "        np.where(\n",
    "            ((df['withexplanationquality'] == 1) & (df['ground_truth'] == 1)) |\n",
    "            ((df['withexplanationquality'] == 0) & (df['ground_truth'] == 0)),\n",
    "            1,\n",
    "            -1\n",
    "        )\n",
    "    )\n",
    "df_map['vf_numeric_llava1.5_with_image_q20_i10_s0'].sample(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Compute M2–M4 using utility, for each applied setting\n",
    "all_scores = []\n",
    "for setting in applied_settings:\n",
    "    df = df_map[setting]\n",
    "    \n",
    "    # M2: Positive conversion rate\n",
    "    #    P(u3 = +1 | u2 = 0, u3 != 0)\n",
    "    mask2 = (df['utility_2'] == 0) & (df['utility_3'] != 0)\n",
    "    m2 = (df.loc[mask2, 'utility_3'] == 1).sum() / mask2.sum()\n",
    "    \n",
    "    # M3: Error recovery rate\n",
    "    #    P(u3 >= 0 | u2 < 0)\n",
    "    mask3 = df['utility_2'] < 0\n",
    "    m3 = (df.loc[mask3, 'utility_3'] >= 0).sum() / mask3.sum()\n",
    "    \n",
    "    # M4: Correct loss rate\n",
    "    #    P(u3 <= 0 | u2 > 0)\n",
    "    mask4 = df['utility_2'] > 0\n",
    "    m4 = (df.loc[mask4, 'utility_3'] <= 0).sum() / mask4.sum()\n",
    "    \n",
    "    all_scores.append({\n",
    "        'Setting': applied_setting_names[setting],\n",
    "        'Positive Conversion': m2,\n",
    "        'Error Recovery': m3,\n",
    "        'Correct Loss': m4\n",
    "    })\n",
    "\n",
    "# 3) Build DataFrame and plot grouped bar chart\n",
    "df_scores = pd.DataFrame(all_scores).set_index('Setting')\n",
    "\n",
    "ax = df_scores.plot(\n",
    "    kind='bar',\n",
    "    figsize=(10, 6),\n",
    "    width=0.6,\n",
    "    edgecolor='k'\n",
    ")\n",
    "ax.set_xlabel('Setting', fontsize=16)\n",
    "ax.set_ylabel('Rate', fontsize=18)\n",
    "ax.set_ylim(0, 1)\n",
    "ax.set_title('Correctness Change Measures Across Settings', fontsize=16)\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=0, fontsize=16)\n",
    "ax.legend(title='Metric', loc='upper right', fontsize=16, title_fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 4) Group by metric on the x‑axis instead of by setting\n",
    "#    i.e. transpose so rows become metrics and columns become settings\n",
    "df_scores_T = df_scores.transpose()\n",
    "\n",
    "ax = df_scores_T.plot(\n",
    "    kind='bar',\n",
    "    figsize=(10, 6),\n",
    "    width=0.6,\n",
    "    edgecolor='k'\n",
    ")\n",
    "ax.set_xlabel('Metric', fontsize=16)\n",
    "ax.set_ylabel('Rate', fontsize=18)\n",
    "ax.set_title('Correctness Change Measures', fontsize=16)\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=0, fontsize=16)\n",
    "ax.legend(title='Setting', loc='upper right', fontsize=16, title_fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) Plot M3 (Error Recovery) for VF Numeric vs VF Descriptive\n",
    "# -------------------------------------------------------------\n",
    "\n",
    "numeric_descriptive_settings_pair = [\n",
    "    ['vf_numeric_llava1.5_with_image_q20_i10_s0', 'vf_descriptive_llava1.5_with_image_q20_i10_s0'],\n",
    "    ['contr_numeric_llava1.5_with_image_q20_i10_s0', 'contr_descriptive_llava1.5_with_image_q20_i10_s0'],\n",
    "    ['showbothmetrics_llava1.5_with_image_q20_i10_s0', 'showbothmetrics_descriptive_llava1.5_with_image_q20_i10_s0'],\n",
    "]\n",
    "\n",
    "# Define group labels\n",
    "group_labels = ['VF', 'CONTR', 'Both']\n",
    "\n",
    "# Compute M3 for each group\n",
    "m3_dict = {}\n",
    "for label, (num_s, des_s) in zip(group_labels, numeric_descriptive_settings_pair):\n",
    "    df_num = df_map[num_s]\n",
    "    df_des = df_map[des_s]\n",
    "    \n",
    "    mask_num = df_num['utility_2'] < 0\n",
    "    mask_des = df_des['utility_2'] < 0\n",
    "    \n",
    "    m3_num = (df_num.loc[mask_num, 'utility_3'] >= 0).sum() / mask_num.sum()\n",
    "    m3_des = (df_des.loc[mask_des, 'utility_3'] >= 0).sum() / mask_des.sum()\n",
    "    \n",
    "    m3_dict[label] = {'Numeric': m3_num, 'Descriptive': m3_des}\n",
    "\n",
    "# Build DataFrame\n",
    "m3_df = pd.DataFrame.from_dict(m3_dict, orient='index')\n",
    "\n",
    "# Plot grouped bars\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "m3_df.plot(\n",
    "    kind='bar',\n",
    "    ax=ax,\n",
    "    width=0.6,\n",
    "    edgecolor='k'\n",
    ")\n",
    "\n",
    "# Tighten spacing\n",
    "ax.margins(x=0.02)\n",
    "\n",
    "# Labels and title\n",
    "ax.set_ylabel('Error Recovery Rate', fontsize=16)\n",
    "# ax.set_ylim(0, 1)\n",
    "ax.set_title('Error Recovery Rate by Numeric vs Descriptive', fontsize=14)\n",
    "ax.set_xticklabels(group_labels, rotation=0, fontsize=16)\n",
    "\n",
    "# Annotate bar values\n",
    "for i, row_label in enumerate(group_labels):\n",
    "    for j, col_label in enumerate(['Numeric', 'Descriptive']):\n",
    "        v = m3_df.loc[row_label, col_label]\n",
    "        ax.text(i + (j-0.5)*0.3, v, f\"{v:.1%}\",\n",
    "                ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "# Legend\n",
    "ax.legend(title='Format', loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Compute M4 for each group\n",
    "m4_dict = {}\n",
    "for label, (num_s, des_s) in zip(group_labels, numeric_descriptive_settings_pair):\n",
    "    df_num = df_map[num_s]\n",
    "    df_des = df_map[des_s]\n",
    "    \n",
    "    mask_num = df_num['utility_2'] > 0\n",
    "    mask_des = df_des['utility_2'] > 0\n",
    "    \n",
    "    m4_num = (df_num.loc[mask_num, 'utility_3'] <= 0).sum() / mask_num.sum()\n",
    "    m4_des = (df_des.loc[mask_des, 'utility_3'] <= 0).sum() / mask_des.sum()\n",
    "    \n",
    "    m4_dict[label] = {'Numeric': m4_num, 'Descriptive': m4_des}\n",
    "# Build DataFrame\n",
    "m4_df = pd.DataFrame.from_dict(m4_dict, orient='index')\n",
    "# Plot grouped bars\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "m4_df.plot(\n",
    "    kind='bar',\n",
    "    ax=ax,\n",
    "    width=0.6,\n",
    "    edgecolor='k'\n",
    ")\n",
    "# Tighten spacing\n",
    "ax.margins(x=0.02)\n",
    "# Labels and title\n",
    "ax.set_ylabel('Correct Loss Rate', fontsize=16)\n",
    "# ax.set_ylim(0, 1)\n",
    "ax.set_title('Correct Loss Rate by Numeric vs Descriptive', fontsize=14)\n",
    "ax.set_xticklabels(group_labels, rotation=0, fontsize=16)\n",
    "# Annotate bar values\n",
    "for i, row_label in enumerate(group_labels):\n",
    "    for j, col_label in enumerate(['Numeric', 'Descriptive']):\n",
    "        v = m4_df.loc[row_label, col_label]\n",
    "        ax.text(i + (j-0.5)*0.3, v, f\"{v:.1%}\",\n",
    "                ha='center', va='bottom', fontsize=10)\n",
    "# Legend\n",
    "ax.legend(title='Format', loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# Compute M5 (Recovery Gap) correctly for each group\n",
    "m5_dict = {}\n",
    "for label, (num_s, des_s) in zip(group_labels, numeric_descriptive_settings_pair):\n",
    "    df_num = df_map[num_s]\n",
    "    df_des = df_map[des_s]\n",
    "    \n",
    "    # Mask for error cases (u2 < 0) and for correct cases (u2 > 0)\n",
    "    mask_num_neg = df_num['utility_2'] < 0\n",
    "    mask_num_pos = df_num['utility_2'] > 0\n",
    "    mask_des_neg = df_des['utility_2'] < 0\n",
    "    mask_des_pos = df_des['utility_2'] > 0\n",
    "    \n",
    "    # M3_num on negative mask, M4_num on positive mask\n",
    "    m3_num = (df_num.loc[mask_num_neg, 'utility_3'] >= 0).sum()   / mask_num_neg.sum()\n",
    "    m4_num = (df_num.loc[mask_num_pos, 'utility_3'] <= 0).sum()   / mask_num_pos.sum()\n",
    "    \n",
    "    # same for descriptive\n",
    "    m3_des = (df_des.loc[mask_des_neg, 'utility_3'] >= 0).sum()   / mask_des_neg.sum()\n",
    "    m4_des = (df_des.loc[mask_des_pos, 'utility_3'] <= 0).sum()   / mask_des_pos.sum()\n",
    "    \n",
    "    m5_dict[label] = {\n",
    "        'Numeric': m3_num - m4_num,\n",
    "        'Descriptive': m3_des - m4_des\n",
    "    }\n",
    "\n",
    "# Build and plot as before\n",
    "m5_df = pd.DataFrame.from_dict(m5_dict, orient='index')\n",
    "ax = m5_df.plot(kind='bar', figsize=(8,5), width=0.6, edgecolor='k')\n",
    "ax.margins(x=0.02)\n",
    "ax.set_ylabel('Recovery Gap', fontsize=16)\n",
    "ax.set_title('Recovery Gap by Numeric vs Descriptive', fontsize=14)\n",
    "ax.set_xticklabels(group_labels, rotation=0, fontsize=16)\n",
    "for i, row_label in enumerate(group_labels):\n",
    "    for j, col_label in enumerate(['Numeric','Descriptive']):\n",
    "        v = m5_df.loc[row_label, col_label]\n",
    "        ax.text(i + (j-0.5)*0.3, v, f\"{v:.1%}\", ha='center', va='bottom', fontsize=10)\n",
    "ax.legend(title='Format', loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the confidence distributions for each setting\n",
    "for setting in settings:\n",
    "    df = df_map[setting]\n",
    "    plt.figure(figsize=(5, 2))\n",
    "    plt.hist(df['confidence'], bins=20, alpha=0.7, label=setting)\n",
    "    plt.xlabel('Confidence Score')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title(f'Confidence Distribution for {setting}')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import linregress\n",
    "import statsmodels.api as sm\n",
    "\n",
    "def bin_stats(df, not_wanted_value):\n",
    "    n = len(df)\n",
    "    if n == 0:\n",
    "        return pd.Series({'p': 0, 'se': 0, 'n': 0})\n",
    "    k = (df['withexplanationquality'] != not_wanted_value).sum()\n",
    "    p = k / n\n",
    "    # Laplace-smoothed se\n",
    "    p_ = (k + 1) / (n + 2)\n",
    "    se_ = sqrt(p_ * (1 - p_) / (n + 2))\n",
    "    return pd.Series({'p': p, 'se': se_, 'n': n})\n",
    "\n",
    "# colors for each selection\n",
    "color_map = {0: 'C0', 1: 'tab:green', 2: 'tab:orange'}\n",
    "\n",
    "for setting, df in df_map.items():\n",
    "    if setting not in settings:\n",
    "        continue\n",
    "\n",
    "    # choose metric label\n",
    "    if 'avg_vf_contr' in setting:\n",
    "        confidence_metric = 'AVERAGE(VF, CONTR)'\n",
    "    elif setting.startswith('vf'):\n",
    "        confidence_metric = 'VF'\n",
    "    elif setting.startswith('contr'):\n",
    "        confidence_metric = 'CONTR'\n",
    "    else:\n",
    "        confidence_metric = ''\n",
    "\n",
    "    # bin edges\n",
    "    n_bins = 5\n",
    "    bin_edges = np.linspace(0.0, 1.0, n_bins + 1)\n",
    "    bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2\n",
    "    width = (bin_edges[1] - bin_edges[0]) * 0.9\n",
    "    \n",
    "    df['conf_bin'] = pd.cut(\n",
    "        df['confidence'],\n",
    "        bins=bin_edges,\n",
    "        include_lowest=True,  # makes 0.0 go into the first bin\n",
    "        right=True            # makes 1.0 go into the last bin\n",
    "    )\n",
    "\n",
    "    # make 3 subplots\n",
    "    fig, axes = plt.subplots(\n",
    "        1, 3,\n",
    "        figsize=(12, 4),\n",
    "        sharey=True\n",
    "    )\n",
    "\n",
    "    for ax, sel in zip(axes, [0, 1, 2]):\n",
    "        df0 = df[df['withexplanation'] == sel]\n",
    "        grouped = (\n",
    "            df0\n",
    "            .groupby('conf_bin', observed=False)\n",
    "            .apply(lambda g: bin_stats(g, sel))\n",
    "            .reset_index()\n",
    "        )\n",
    "        # to percentages\n",
    "        grouped['p']  *= 100\n",
    "        grouped['se'] *= 100\n",
    "\n",
    "        ax.bar(\n",
    "            bin_centers,\n",
    "            grouped['p'],\n",
    "            width=width,\n",
    "            yerr=grouped['se'],\n",
    "            capsize=5,\n",
    "            edgecolor='black',\n",
    "            color=color_map[sel]\n",
    "        )\n",
    "        # annotate counts on top of each bar\n",
    "        for x, height, count in zip(bin_centers, grouped['p'], grouped['n']):\n",
    "            ax.text(\n",
    "                x, \n",
    "                height + 1,              # small vertical offset\n",
    "                f'n={int(count)}',\n",
    "                ha='center', \n",
    "                va='bottom',\n",
    "                fontsize='small'\n",
    "            )\n",
    "                \n",
    "        # ---------------------------------------------------------------\n",
    "        # Weighted Linear Regression using WLS (weights = counts, n)\n",
    "        # ---------------------------------------------------------------\n",
    "        x_data = bin_centers\n",
    "        y_data = grouped['p']\n",
    "        weights = grouped['n']\n",
    "        \n",
    "        # Remove potential bins with zero counts, which could be problematic\n",
    "        valid = weights > 0\n",
    "        x_valid = x_data[valid]\n",
    "        y_valid = y_data[valid]\n",
    "        weights_valid = weights[valid]\n",
    "        \n",
    "        # Only regress if we have 2+ points\n",
    "        if len(x_valid) > 1:\n",
    "            X = sm.add_constant(x_valid)\n",
    "            wls_model = sm.WLS(y_valid, X, weights=weights_valid)\n",
    "            results = wls_model.fit()\n",
    "            intercept, slope = results.params\n",
    "            slope_se = results.bse[1]  # standard error of slope\n",
    "            # Create line for plotting\n",
    "            x_line = np.linspace(min(x_valid), max(x_valid), 50)\n",
    "            y_line = slope * x_line + intercept\n",
    "            ax.plot(\n",
    "                x_line, y_line,\n",
    "                color='red', linestyle='--',\n",
    "                label=(f\"y = {slope:.2f}x + {intercept:.2f}\\n\"\n",
    "                    f\"Slope SE = {slope_se:.2f}\")\n",
    "            )\n",
    "\n",
    "            ax.legend(fontsize='small')\n",
    "\n",
    "        # ---------------------------------------------------------------\n",
    "        \n",
    "\n",
    "        ax.set_title(f's₂ == {sel}')\n",
    "        ax.set_xlabel(f'Confidence ({confidence_metric})')\n",
    "        ax.set_xlim(0, 1)\n",
    "\n",
    "\n",
    "        \n",
    "    axes[0].set_ylabel(f'P(s₃ ≠ s₂ ∣ s₂ == {{0,1,2}}) [%]')\n",
    "    axes[0].set_ylim(0, 100)\n",
    "    plt.suptitle(f\"Setting: {setting}\")\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.90])\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "\n",
    "def bin_stats(df, not_wanted_value):\n",
    "    n = len(df)\n",
    "    if n == 0:\n",
    "        return pd.Series({'p': 0, 'se': 0, 'n': 0})\n",
    "    k = (df['withexplanationquality'] != not_wanted_value).sum()\n",
    "    p = k / n\n",
    "    # Laplace-smoothed standard error\n",
    "    p_ = (k + 1) / (n + 2)\n",
    "    se_ = sqrt(p_ * (1 - p_) / (n + 2))\n",
    "    return pd.Series({'p': p, 'se': se_, 'n': n})\n",
    "\n",
    "metrics_to_plot = ['vf_numeric_llava1.5_with_image_q20_i10_s0', 'vf_descriptive_llava1.5_with_image_q20_i10_s0']\n",
    "metric_colors = {'VF_numeric': 'C0', 'VF_descriptive': 'C1'}\n",
    "# a simple mapping from your metric‐IDs to pretty legend labels\n",
    "label_map = {\n",
    "    'vf_numeric_llava1.5_with_image_q20_i10_s0': 'VF numeric',\n",
    "    'vf_descriptive_llava1.5_with_image_q20_i10_s0': 'VF descriptive',\n",
    "}\n",
    "\n",
    "n_bins = 5\n",
    "bin_edges = np.linspace(0.0, 1.0, n_bins + 1)\n",
    "bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12, 4), sharey=True)\n",
    "# Loop over the s₂ values – here using withexplanation == 0, 1, 2\n",
    "for ax, sel in zip(axes, [0, 1, 2]):\n",
    "    # For each s₂ subplot, plot both metrics on the same axes\n",
    "    for metric in metrics_to_plot:\n",
    "        # Get the corresponding DataFrame from df_map.\n",
    "        # (For safety, we use a copy since we will add a new column.)\n",
    "        df = df_map[metric].copy()\n",
    "        # Bin the confidence values (you can adjust include_lowest/right as appropriate)\n",
    "        df['conf_bin'] = pd.cut(\n",
    "            df['confidence'],\n",
    "            bins=bin_edges,\n",
    "            include_lowest=True,\n",
    "            right=True\n",
    "        )\n",
    "        \n",
    "        # Select the data for the current s₂ value\n",
    "        df0 = df[df['withexplanation'] == sel]\n",
    "        \n",
    "        # Group by confidence bin and calculate proportion (p), laplace-smoothed se, and count (n)\n",
    "        grouped = (df0.groupby('conf_bin', observed=False)\n",
    "                    .apply(lambda g: bin_stats(g, sel))\n",
    "                    .reset_index())\n",
    "        # (Optional) make sure that the results are ordered by bin\n",
    "        grouped = grouped.sort_values(by='conf_bin')\n",
    "        \n",
    "        # Convert proportions to percentages\n",
    "        grouped['p'] *= 100\n",
    "        grouped['se'] *= 100\n",
    "        \n",
    "        # Plot a line with markers and error bars using errorbar()\n",
    "        ax.errorbar(\n",
    "            bin_centers,\n",
    "            grouped['p'],\n",
    "            yerr=grouped['se'],\n",
    "            marker='o',\n",
    "            linestyle='-',\n",
    "            color=metric_colors.get(metric, None),\n",
    "            capsize=5,\n",
    "            label=label_map[metric]\n",
    "        )\n",
    "        \n",
    "        # annotate counts on top of each bar\n",
    "        height_adjust = -1 if metric == 'vf_numeric_llava1.5_with_image_q20_i10_s0' else 4\n",
    "        for x, height, count in zip(bin_centers, grouped['p'], grouped['n']):\n",
    "            ax.text(\n",
    "                x + 0.08,              # small horizontal offset\n",
    "                height + height_adjust if height < 99 else height - 10 + height_adjust,              # small vertical offset\n",
    "                f'n={int(count)}',\n",
    "                ha='center',\n",
    "                va='bottom',\n",
    "                fontsize='small',\n",
    "                color=metric_colors['VF_numeric'] if 'numeric' in metric else metric_colors['VF_descriptive']\n",
    "            )\n",
    "        \n",
    "\n",
    "    title = f\"s{sel}: \"\n",
    "    if sel == 0:\n",
    "        title += \"User thinks AI is Correct\"\n",
    "    elif sel == 1:\n",
    "        title += \"User thinks AI is Incorrect\"\n",
    "    elif sel == 2:\n",
    "        title += \"User is Unsure\"\n",
    "    ax.set_title(f'{title}')\n",
    "    ax.set_xlabel('Confidence')\n",
    "    ax.set_xlim(0, 1)\n",
    "    # Set legend name\n",
    "    # ax.legend(fontsize='small')\n",
    "    \n",
    "handles, labels = axes[0].get_legend_handles_labels()\n",
    "fig.legend(handles, labels, ncol=3, fontsize='small')\n",
    "axes[0].set_ylabel('Switch Rate at Stage 3 [%]', fontsize=14)\n",
    "axes[0].set_ylim(0, 100)\n",
    "plt.suptitle(\"After seeing explanation...\", y=0.96)\n",
    "plt.tight_layout(rect=[0, 0, 1, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional color mapping for compared metrics\n",
    "compared_metric_colors = {\n",
    "'vf_numeric_llava1.5_with_image_q20_i10_s0': 'C0',\n",
    "'contr_numeric_llava1.5_with_image_q20_i10_s0': 'C2',\n",
    "'avg_vf_contr_numeric_llava1.5_with_image_q20_i10_s0': 'C3'\n",
    "}\n",
    "\n",
    "compared_metrics2 = ['vf_numeric_llava1.5_with_image_q20_i10_s0', 'contr_numeric_llava1.5_with_image_q20_i10_s0', 'avg_vf_contr_numeric_llava1.5_with_image_q20_i10_s0']\n",
    "\n",
    "\n",
    "# Update label_map if needed\n",
    "label_map.update({\n",
    "'contr_numeric_llava1.5_with_image_q20_i10_s0': 'Contr. numeric',\n",
    "'avg_vf_contr_numeric_llava1.5_with_image_q20_i10_s0': 'Avg VF Contr. numeric'\n",
    "})\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12, 4), sharey=True)\n",
    "\n",
    "LABEL_PAD   = 2      # vertical step (points) you’re willing to move the label\n",
    "MIN_SEP_Y   = 3      # how close two labels can be before we call it a collision\n",
    "MIN_SEP_X   = 0.15   # so labels on different bars at same x don’t count\n",
    "placed_xy   = []     # keeps track of every label we’ve already drawn\n",
    "\n",
    "def find_free_y(x, y0):\n",
    "    \"\"\"\n",
    "    Return the nearest y‑coordinate to y0 that is not 'too close' to any\n",
    "    existing label.  We look up, then down, in LABEL_PAD‑point steps.\n",
    "    \"\"\"\n",
    "    up_y   = y0\n",
    "    down_y = y0\n",
    "    # keep expanding the search ring until we find an empty slot\n",
    "    while True:\n",
    "        # try above first\n",
    "        if not any(abs(up_y   - py) < MIN_SEP_Y and abs(x-px) < MIN_SEP_X for px, py in placed_xy):\n",
    "            placed_xy.append((x, up_y))\n",
    "            return up_y\n",
    "        # then one step below\n",
    "        if not any(abs(down_y - py) < MIN_SEP_Y and abs(x-px) < MIN_SEP_X for px, py in placed_xy):\n",
    "            placed_xy.append((x, down_y))\n",
    "            return down_y\n",
    "        up_y   += LABEL_PAD\n",
    "        down_y -= LABEL_PAD\n",
    "\n",
    "\n",
    "# Loop over the s₂ values – here using withexplanation values 0, 1, 2\n",
    "for ax, sel in zip(axes, [0, 1, 2]):\n",
    "    # First, plot the original metrics\n",
    "    for metric in compared_metrics2:\n",
    "        df = df_map[metric].copy()\n",
    "        df['conf_bin'] = pd.cut(\n",
    "        df['confidence'],\n",
    "        bins=bin_edges,\n",
    "        include_lowest=True,\n",
    "        right=True\n",
    "        )\n",
    "        df0 = df[df['withexplanation'] == sel]\n",
    "        grouped = (df0.groupby('conf_bin', observed=False)\n",
    "            .apply(lambda g: bin_stats(g, sel))\n",
    "            .reset_index())\n",
    "        grouped = grouped.sort_values(by='conf_bin')\n",
    "        grouped['p'] *= 100\n",
    "        grouped['se'] *= 100\n",
    "\n",
    "        valid = grouped['n'] > 0\n",
    "        ax.errorbar(\n",
    "            bin_centers[valid],\n",
    "            grouped.loc[valid, 'p'],\n",
    "            yerr=grouped.loc[valid, 'se'],\n",
    "            marker='o',\n",
    "            linestyle='-',\n",
    "            color=compared_metric_colors.get(metric, None),\n",
    "            capsize=5,\n",
    "            label=label_map[metric]\n",
    "        )\n",
    "        \n",
    "        # annotate counts\n",
    "        # ── 2. inside your loop that writes the labels ─────────────────────────────────\n",
    "        for x, height, count in zip(bin_centers, grouped['p'], grouped['n']):\n",
    "            if count == 0:\n",
    "                continue\n",
    "            # first decide the nominal y you would have used\n",
    "            desired_y = height\n",
    "            if height > 95:\n",
    "                desired_y = height - 5\n",
    "            elif height < 5:\n",
    "                desired_y = height + 5\n",
    "\n",
    "            \n",
    "            # ask the helper for a collision‑free y position\n",
    "            y_for_label = find_free_y(x + 0.08, desired_y)\n",
    "            \n",
    "            ax.text(\n",
    "                x + 0.08,\n",
    "                y_for_label,\n",
    "                f'n={int(count)}',\n",
    "                ha='center',\n",
    "                va='bottom',\n",
    "                fontsize='small',\n",
    "                color=compared_metric_colors.get(metric, 'black')\n",
    "            )\n",
    "\n",
    "            \n",
    "    # Set subplot titles and labels\n",
    "    title = f\"s{sel}: \"\n",
    "    if sel == 0:\n",
    "        title += \"User thinks AI is Correct\"\n",
    "    elif sel == 1:\n",
    "        title += \"User thinks AI is Incorrect\"\n",
    "    elif sel == 2:\n",
    "        title += \"User is Unsure\"\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel('Confidence')\n",
    "    ax.set_xlim(0, 1)\n",
    "    # ax.legend(fontsize='small')\n",
    "    \n",
    "handles, labels = axes[0].get_legend_handles_labels()\n",
    "fig.legend(handles, labels, ncol=3, fontsize='small')\n",
    "axes[0].set_ylabel('Switch Rate at Stage 3 [%]', fontsize=14)\n",
    "axes[0].set_ylim(0, 100) \n",
    "plt.suptitle(\"After seeing explanation...\") \n",
    "plt.tight_layout(rect=[0, 0, 1, 1]) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
