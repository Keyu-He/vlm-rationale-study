{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting numpy\n",
      "  Downloading numpy-1.24.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 17.3 MB 15.5 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: numpy\n",
      "Successfully installed numpy-1.24.4\n",
      "\u001b[33mWARNING: You are using pip version 20.2.3; however, version 25.0.1 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing setting: showbothmetrics_llava1.5_with_image_q20_i10_s0\n",
      "30 participants in showbothmetrics_llava1.5_with_image_q20_i10_s0\n",
      "Processing setting: prodmetric_llava1.5_with_image_q20_i10_s0\n",
      "31 participants in prodmetric_llava1.5_with_image_q20_i10_s0\n",
      "Processing setting: vf_numeric_llava1.5_with_image_q20_i10_s0\n",
      "28 participants in vf_numeric_llava1.5_with_image_q20_i10_s0\n",
      "\n",
      "Overlap between settings:\n",
      "Overlap between showbothmetrics_llava1.5_with_image_q20_i10_s0 and prodmetric_llava1.5_with_image_q20_i10_s0: 0 participants\n",
      "Overlap between showbothmetrics_llava1.5_with_image_q20_i10_s0 and vf_numeric_llava1.5_with_image_q20_i10_s0: 1 participants\n",
      "Overlap between prodmetric_llava1.5_with_image_q20_i10_s0 and vf_numeric_llava1.5_with_image_q20_i10_s0: 0 participants\n",
      "\n",
      "Participants present in all three settings: 0\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import glob, os\n",
    "from itertools import combinations\n",
    "\n",
    "USER_STUDIES_DIR = \"/home/shared/vlm_rationales_eval/user_studies_data/\"\n",
    "settings = [\n",
    "    'showbothmetrics_llava1.5_with_image_q20_i10_s0',\n",
    "    'prodmetric_llava1.5_with_image_q20_i10_s0',\n",
    "    'vf_numeric_llava1.5_with_image_q20_i10_s0'\n",
    "]\n",
    "\n",
    "# Dictionary to hold the set of user_ids for each setting.\n",
    "settings_user_ids = {}\n",
    "\n",
    "for setting in settings:\n",
    "    print(f\"Processing setting: {setting}\")\n",
    "    pattern = os.path.join(USER_STUDIES_DIR, \"prolific_batches\", \"batch_interaction_data\", setting, \"*.json\")\n",
    "    files = glob.glob(pattern)\n",
    "    user_ids = set()\n",
    "    \n",
    "    for file in files:\n",
    "        with open(file) as f:\n",
    "            batch_data = json.load(f)\n",
    "            # Add all user_ids from the current JSON file.\n",
    "            user_ids.update(batch_data.keys())\n",
    "    \n",
    "    settings_user_ids[setting] = user_ids\n",
    "    print(f\"{len(user_ids)} participants in {setting}\")\n",
    "\n",
    "# Compare overlaps between each pair of settings.\n",
    "print(\"\\nOverlap between settings:\")\n",
    "for s1, s2 in combinations(settings, 2):\n",
    "    overlap = settings_user_ids[s1].intersection(settings_user_ids[s2])\n",
    "    print(f\"Overlap between {s1} and {s2}: {len(overlap)} participants\")\n",
    "    \n",
    "# Optionally, check for participants common to all three settings.\n",
    "triple_overlap = settings_user_ids[settings[0]].intersection(settings_user_ids[settings[1]], settings_user_ids[settings[2]])\n",
    "print(f\"\\nParticipants present in all three settings: {len(triple_overlap)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel died. Error: /usr/bin/python: No module named ipykernel_launcher... View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import glob, os\n",
    "\n",
    "USER_STUDIES_DIR = \"/home/shared/vlm_rationales_eval/user_studies_data/\"\n",
    "\n",
    "# setting = 'showbothmetrics_llava1.5_with_image_q20_i10_s0'\n",
    "# setting = 'prodmetric_llava1.5_with_image_q20_i10_s0'\n",
    "setting = 'vf_numeric_llava1.5_with_image_q20_i10_s0'\n",
    "print(f\"Setting: {setting}\")\n",
    "files = glob.glob(f'{USER_STUDIES_DIR}/prolific_batches/batch_interaction_data/{setting}/*.json')\n",
    "data = {}\n",
    "for file in files:\n",
    "    with open(file) as f:\n",
    "        batch_data = json.load(f)\n",
    "        for user_id, session in batch_data.items():\n",
    "            if user_id in data:\n",
    "                data[user_id].append(session)  # Append to list instead of overwriting\n",
    "            else:\n",
    "                data[user_id] = [session]  # Initialize as list\n",
    "\n",
    "print(f\"{sum(len(sessions) for sessions in data.values())} total sessions loaded from {len(files)} files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "310\n"
     ]
    }
   ],
   "source": [
    "all_instances = sum([session['interactions'] for sessions in data.values() for session in sessions], [])\n",
    "print(len(all_instances))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage                   \tUnsure Rate\tTotalAcc\tNotUnsureAcc\tUtility\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Answer Only              \t71.3% ± 5.0%\t16.5% ± 4.1%\t57.3% ± 10.3%\t0.042 ± 0.060\n",
      "copiable results: 71.3% ± 5.0%, 16.5% ± 4.1%, 57.3% ± 10.3%, 0.042 ± 0.060\n",
      "With Explanation         \t33.2% ± 5.2%\t37.1% ± 5.4%\t55.6% ± 6.8%\t0.074 ± 0.091\n",
      "copiable results: 33.2% ± 5.2%, 37.1% ± 5.4%, 55.6% ± 6.8%, 0.074 ± 0.091\n",
      "With Explanation + Quality\t10.3% ± 3.4%\t57.7% ± 5.5%\t64.4% ± 5.6%\t0.258 ± 0.102\n",
      "copiable results: 10.3% ± 3.4%, 57.7% ± 5.5%, 64.4% ± 5.6%, 0.258 ± 0.102\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the interactions\n",
    "from collections import Counter\n",
    "import math\n",
    "\n",
    "def compute_margin(p, n):\n",
    "    return 1.96*math.sqrt(p*(1-p)/n) if n > 0 else 0\n",
    "\n",
    "def compute_proportion_ci(p, n):\n",
    "    margin = compute_margin(p, n)\n",
    "    return (p - margin, p + margin)\n",
    "\n",
    "def compute_mean_ci(values):\n",
    "    n = len(values)\n",
    "    mean_val = np.mean(values)\n",
    "    std_val = np.std(values, ddof=1)\n",
    "    se = std_val/np.sqrt(n) if n > 0 else 0\n",
    "    return (mean_val - 1.96*se, mean_val + 1.96*se)\n",
    "\n",
    "def evaluate_answers(stage, instances):\n",
    "    ground_truths = np.array([1-x['question']['prediction_is_correct'] for x in instances])       # 0 means AI is correct, 1 means AI is incorrect\n",
    "    preds = np.array([x['user_selections'][stage] for x in instances])\n",
    "    true_positives = np.sum(np.logical_and(preds == 0, ground_truths == 0))\n",
    "    false_positives = np.sum(np.logical_and(preds == 0, ground_truths == 1))\n",
    "    true_negatives = np.sum(np.logical_and(preds == 1, ground_truths == 1))\n",
    "    false_negatives = np.sum(np.logical_and(preds == 1, ground_truths == 0))\n",
    "\n",
    "    unsure_rate = np.mean(preds == 2)\n",
    "    accuracy = (true_positives + true_negatives) / (true_positives + false_positives + true_negatives + false_negatives)\n",
    "    total_accuracy = (true_positives + true_negatives) / len(ground_truths)\n",
    "    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives > 0 else 0\n",
    "    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0\n",
    "    false_positive_rate = false_positives / (false_positives + true_negatives) if false_positives + true_negatives > 0 else 0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if precision + recall > 0 else 0\n",
    "\n",
    "    # Utility: 0 if user is unsure, 1 if user correctly predicts AI correctness, -1 if user incorrectly predicts AI correctness\n",
    "    individual_utilities = np.array([0 if preds[i] == 2 else 1-2*np.abs(preds[i] - ground_truths[i]) for i in range(len(preds))])\n",
    "    utility = np.mean(individual_utilities)\n",
    "    \n",
    "    # Compute 95% CIs.\n",
    "    total_accuracy_ci = compute_proportion_ci(total_accuracy, len(ground_truths))\n",
    "    unsure_rate_ci    = compute_proportion_ci(unsure_rate, len(preds))\n",
    "    \n",
    "    non_unsure = preds != 2\n",
    "    non_unsure_n = np.sum(non_unsure)\n",
    "    accuracy_ci = compute_proportion_ci(accuracy, non_unsure_n) if non_unsure_n > 0 else (0, 0)\n",
    "    \n",
    "    precision_n = true_positives + false_positives\n",
    "    precision_ci = compute_proportion_ci(precision, precision_n) if precision_n > 0 else (0, 0)\n",
    "    \n",
    "    recall_n = true_positives + false_negatives\n",
    "    recall_ci = compute_proportion_ci(recall, recall_n) if recall_n > 0 else (0, 0)\n",
    "    \n",
    "    fpr_n = false_positives + true_negatives\n",
    "    fpr_ci = compute_proportion_ci(false_positive_rate, fpr_n) if fpr_n > 0 else (0, 0)\n",
    "    \n",
    "    utility_ci = compute_mean_ci(individual_utilities)\n",
    "\n",
    "    return {\n",
    "        'total_accuracy': total_accuracy,\n",
    "        'total_accuracy_ci': total_accuracy_ci,\n",
    "        'accuracy': accuracy,\n",
    "        'accuracy_ci': accuracy_ci,\n",
    "        'precision': precision,\n",
    "        'precision_ci': precision_ci,\n",
    "        'recall': recall,\n",
    "        'recall_ci': recall_ci,\n",
    "        'false_positive_rate': false_positive_rate,\n",
    "        'fpr_ci': fpr_ci,\n",
    "        'f1': f1,\n",
    "        'unsure_rate': unsure_rate,\n",
    "        'unsure_rate_ci': unsure_rate_ci,\n",
    "        'utility': utility,\n",
    "        'utility_ci': utility_ci,\n",
    "        'preds': preds,\n",
    "        'ground_truths': ground_truths,\n",
    "        'individual_utilities': individual_utilities,\n",
    "    }\n",
    "\n",
    "answeronly_results = evaluate_answers('answeronly', all_instances)\n",
    "withexplanation_results = evaluate_answers('withexplanation', all_instances)\n",
    "withexplanationquality_results = evaluate_answers('withexplanationquality', all_instances)\n",
    "\n",
    "# print(\"Stage                   \\tUnsure Rate\\tTotalAcc\\tNotUnsureAcc\\tPrecision\\tRecall\\t\\tF1\\t\\tFPR\\t\\tUtility\")\n",
    "# print(\"-\"*140)\n",
    "# for stage, results in zip(\n",
    "#     ['Answer Only', 'With Explanation', 'With Explanation + Quality'], \n",
    "#     [answeronly_results, withexplanation_results, withexplanationquality_results]\n",
    "# ):\n",
    "#     print(f\"{stage:<25}\\t{results['unsure_rate']:.1%}\\t\\t{results['total_accuracy']:.1%}\\t\\t{results['accuracy']:.1%}\\t\\t{results['precision']:.3f}\\t\\t{results['recall']:.3f}\\t\\t{results['f1']:.3f}\\t\\t{results['false_positive_rate']:.3f}\\t\\t{results['utility']:.3f}\")\n",
    "    \n",
    "# print(\"Stage                   \\tUnsure Rate\\tTotalAcc\\tNotUnsureAcc\\tPrecision\\tRecall\\t\\tF1\\tFPR\\t\\tUtility\")\n",
    "print(\"Stage                   \\tUnsure Rate\\tTotalAcc\\tNotUnsureAcc\\tUtility\")\n",
    "# print(\"-\"*150) \n",
    "print(\"-\"*100)\n",
    "for stage_name, results in zip(\n",
    "    ['Answer Only', 'With Explanation', 'With Explanation + Quality'], \n",
    "    [answeronly_results, withexplanation_results, withexplanationquality_results]\n",
    "):\n",
    "    # Calculate margins as half the width of each CI.\n",
    "    unsure_margin    = (results['unsure_rate_ci'][1] - results['unsure_rate_ci'][0]) / 2\n",
    "    total_acc_margin = (results['total_accuracy_ci'][1] - results['total_accuracy_ci'][0]) / 2\n",
    "    acc_margin       = (results['accuracy_ci'][1] - results['accuracy_ci'][0]) / 2\n",
    "    prec_margin      = (results['precision_ci'][1] - results['precision_ci'][0]) / 2 if results['precision_ci'] != (0, 0) else 0\n",
    "    recall_margin    = (results['recall_ci'][1] - results['recall_ci'][0]) / 2 if results['recall_ci'] != (0, 0) else 0\n",
    "    fpr_margin       = (results['fpr_ci'][1] - results['fpr_ci'][0]) / 2 if results['fpr_ci'] != (0, 0) else 0\n",
    "    utility_margin   = (results['utility_ci'][1] - results['utility_ci'][0]) / 2\n",
    "\n",
    "    # For proportion metrics, we print as percentages; f1 is printed as a decimal (CI not computed); utility remains as a number.\n",
    "    print(f\"{stage_name:<25}\\t\"\n",
    "          f\"{results['unsure_rate']:.1%} ± {unsure_margin:.1%}\\t\"\n",
    "          f\"{results['total_accuracy']:.1%} ± {total_acc_margin:.1%}\\t\"\n",
    "          f\"{results['accuracy']:.1%} ± {acc_margin:.1%}\\t\"\n",
    "        #   f\"{results['precision']:.1%} ± {prec_margin:.1%}\\t\"\n",
    "        #   f\"{results['recall']:.1%} ± {recall_margin:.1%}\\t\"\n",
    "        #   f\"{results['f1']:.3f}\\t\"\n",
    "        #   f\"{results['false_positive_rate']:.1%} ± {fpr_margin:.1%}\\t\"\n",
    "          f\"{results['utility']:.3f} ± {utility_margin:.3f}\")\n",
    "    \n",
    "    print(\"copiable results:\", end=' ')\n",
    "    print(f\"{results['unsure_rate']:.1%} ± {unsure_margin:.1%}, \", end='')\n",
    "    print(f\"{results['total_accuracy']:.1%} ± {total_acc_margin:.1%}, \", end='')\n",
    "    print(f\"{results['accuracy']:.1%} ± {acc_margin:.1%}, \", end='')\n",
    "    print(f\"{results['utility']:.3f} ± {utility_margin:.3f}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
